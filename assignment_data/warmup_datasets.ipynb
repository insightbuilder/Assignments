{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kindly review the Generators, List, Dictionary warmups\n",
    "\n",
    "# Preprocessing involves visualising how the data transforms, \n",
    "# followed by remembering and choosing correct methods & then implementing.\n",
    "\n",
    "# Practicing writing functions that takes different types of args, and \n",
    "# managing the processing inside the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\reinforce\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import (\n",
    "    load_dataset,\n",
    "    list_datasets,\n",
    "    load_dataset_builder,\n",
    "    get_dataset_split_names,\n",
    "    get_dataset_infos,\n",
    "    get_dataset_config_info,\n",
    "    get_dataset_config_names,\n",
    "    load_from_disk,\n",
    "    list_metrics,\n",
    "    load_metric\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list = list_datasets()\n",
    "\n",
    "# Choose 2 different datasets and do the below warmups for 2 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = list_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, x in enumerate(ds_list):\n",
    "    print(x)\n",
    "    if ind > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, x in enumerate(metrics_list):\n",
    "    print(x)\n",
    "    if ind > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we might want the info on dataset before pulling it to locally\n",
    "ds_info = get_dataset_infos('rotten_tomatoes')\n",
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the split names\n",
    "ds_splits = get_dataset_split_names('rotten_tomatoes')\n",
    "ds_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_config_names = get_dataset_config_names(\"glue\")\n",
    "glue_config_names\n",
    "\n",
    "# Try the above with \"PolyAI/minds14\"\n",
    "# Try to locate more datasets with such configs\n",
    "# https://huggingface.co/datasets/PolyAI/minds14\n",
    "# https://huggingface.co/datasets/nyu-mll/glue/viewer/\n",
    "# https://gluebenchmark.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue = load_dataset_builder(\"glue\", 'cola')\n",
    "# Please pick one among the available configs: ['ax', 'cola', 'mnli', 'mnli_matched', 'mnli_mismatched', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glue.builder_configs)  # speaks about the parquet / arrow file at disk level details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glue.info)  # discusses the parts of the Dataset at datalevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue.cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomatoes_ds = load_dataset(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with repo with remote code\n",
    "c4_config_names = get_dataset_config_names(\"c4\", trust_remote_code=True)\n",
    "c4_config_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c4_splits = get_dataset_split_names(\"c4\", \"realnewslike\", trust_remote_code=True)\n",
    "c4_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diving into the data part of the dataset, working with rotten_tomatoes\n",
    "\n",
    "tomatoes_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe the ds_obj is DatasetDict, so need to access the Dataset object inside using the splits\n",
    "toma_train_ds = tomatoes_ds['train']\n",
    "toma_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try exploring the above Dataset, DatasetDict, DatasetInfo, DatasetConfig objects with dir function\n",
    "toma_train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toma_train_ds['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing operations. Whats the difference?\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "d = toma_train_ds[0]['text']  # accessing the rows is faster\n",
    "print(time.time() - start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "d = toma_train_ds['text'][0]   # accessing the columns will take time\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a iterator from dataset\n",
    "toma_train_iter = load_dataset(\"rotten_tomatoes\", split='train', streaming=True)\n",
    "# streaming is getting data when required\n",
    "for ex in toma_train_iter:\n",
    "    print(ex)\n",
    "    break\n",
    "# The iter will have data still... check below \n",
    "# this is different from the iter object that we create \n",
    "# by iter() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(toma_train_iter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(toma_train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(toma_train_iter.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toma_train_ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = load_dataset(path=\"billsum\",\n",
    "                       split=\"train\",\n",
    "                       keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue = load_dataset(\"glue\", \"mrpc\",\n",
    "                    cache_dir=\"C:\\\\Users\\\\kamal\\\\.cache\\\\huggingface\\\\\",\n",
    "                    download_mode='reuse_dataset_if_exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_list = [\"emoji\", \"emotion\", \"hate\", \"irony\", \n",
    "\"offensive\", \"sentiment\", \"stance_abortion\", \"stance_atheism\", \n",
    "\"stance_climate\", \"stance_feminist\", \"stance_hillary\"]\n",
    "# Think how else can we get the above list!!!\n",
    "\n",
    "for task in twitter_list:\n",
    "    twitter = load_dataset(\"tweet_eval\", task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Advanced] Write a wrapper Class that takes a dataset name, provides access to list of configs, splits, dataset info, \n",
    "# contains methods to load dataset into memory, and process it as required.  << Expect to spend atleast 60 to 90 mins\n",
    "ought_list = ['ade_corpus_v2', 'banking_77', 'terms_of_service', \n",
    "              'tai_safety_research', 'neurips_impact_statement_risks',\n",
    "              'overruling', 'systematic_review_inclusion', 'one_stop_english',\n",
    "              'tweet_eval_hate', 'twitter_complaints', 'semiconductor_org_types']\n",
    "for task in ought_list:\n",
    "    dataset = load_dataset(\"ought/raft\", task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to time the process\n",
    "import os; import psutil; import timeit\n",
    "\n",
    "mem_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "ade_data = load_dataset(\"ade_corpus_v2\", 'Ade_corpus_v2_classification', split=\"train\")\n",
    "mem_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "\n",
    "mem_after - mem_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=16000,\n",
    "        padding=True,\n",
    "        max_length=100000,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_10 = dataset.select(range(10)).map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_10[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_10 = dataset_10.rename_column(\"intent_class\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import IPython.display\n",
    "\n",
    "# Load the audio file\n",
    "audio_data, sample_rate = librosa.load('example_audio.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the audio file\n",
    "IPython.display.Audio(audio_data, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on text processing\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(toma_train_ds[0][\"text\"])) # explore with different tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    return tokenizer(example[\"text\"])\n",
    "\n",
    "toma_5_ds = toma_train_ds.shuffle(57).select(range(5)).map(tokenization, batched=True)\n",
    "toma_5_ds  # explore the output using know python commands << 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction and au'umenting Images\n",
    "image_tokenizer = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beans_ds = load_dataset(\"beans\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beans_ds['image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomRotation\n",
    "\n",
    "rotate = RandomRotation(degrees=(0, 90))\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [rotate(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beans_ds.set_transform(transforms)\n",
    "beans_ds[0][\"pixel_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diving into processing part. \n",
    "\n",
    "c4_subset = load_dataset(\"allenai/c4\", data_files=[\"en/c4-train.00000-of-01024.json.gz\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c4_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dataset = load_dataset(\"csv\", data_files=\"winequality.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dataset = load_dataset(\"json\", data_files=\"ordvJNeMjPIcomments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": base_url + \"train-v1.1.json\", \"validation\": base_url + \"dev-v1.1.json\"}, field=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/\"\n",
    "data_files = {\"train\": base_url + \"wikipedia-train.parquet\"}\n",
    "wiki = load_dataset(\"parquet\", data_files=data_files, split=\"train\")\n",
    "# Downloading data:   5%|▌         | 977M/18.3G [04:36<1:21:58, 3.53MB/s]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/\"\n",
    "data_files = {\"train\": base_url + \"wikipedia-train.arrow\"}\n",
    "wiki = load_dataset(\"arrow\", data_files=data_files, split=\"train\")\n",
    "# Downloading data:   0%|          | 3.49M/18.3G [00:02<3:36:14, 1.41MB/s] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "my_dict = {\"a\": [1, 2, 3]}  # from dict\n",
    "dataset = Dataset.from_dict(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [{\"a\": 1}, {\"a\": 2}, {\"a\": 3}]  # list of dicts\n",
    "dataset = Dataset.from_list(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_gen():\n",
    "    for i in range(1, 4):\n",
    "        yield {\"a\": i}\n",
    "dataset = Dataset.from_generator(my_gen)  # from generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3]})\n",
    "dataset = Dataset.from_pandas(df)  # from dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entering into Processing of Datasets\n",
    "\n",
    "- Reorder rows and split the dataset.\n",
    "\n",
    "- Rename and remove columns, and other common column operations.\n",
    "\n",
    "- Apply processing functions to each example in a dataset.\n",
    "\n",
    "- Concatenate datasets.\n",
    "\n",
    "- Apply a custom formatting transform.\n",
    "\n",
    "- Save and export processed datasets.\n",
    "\n",
    "    > Methods to master:\n",
    "\n",
    "    Sort, shuffle, select, split, and shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['label'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ds = dataset.sort(\"label\")\n",
    "sorted_ds['label'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting specific indices\n",
    "\n",
    "small_dataset = dataset.select([0, 10, 20, 30, 40, 50])\n",
    "\n",
    "small_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_with_ar = dataset.filter(lambda example: example[\"sentence1\"].startswith(\"Ar\"))\n",
    "start_with_ar['sentence1'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ds = dataset.train_test_split(test_size=0.1)\n",
    "split_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_ds = dataset.shard(num_shards=4, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ds.shuffle(5).select(range(10))['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove, Rename\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sent1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename_column('sentence1', 'sent1')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sent1', 'sentence2', 'label'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns(['idx'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Value\n",
    "\n",
    "practice_cast = dataset.features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 3668/3668 [00:00<00:00, 11877.82 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sent1', 'sentence2', 'label'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "practice_cast['label'] = ClassLabel(names=['Super', 'Dooper'])\n",
    "dataset = dataset.cast(practice_cast)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['Super', 'Dooper'], id=None)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'title': Value(dtype='string', id=None),\n",
       " 'context': Value(dtype='string', id=None),\n",
       " 'question': Value(dtype='string', id=None),\n",
       " 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten : nested Dicts can be flattened\n",
    "\n",
    "squad_ds = load_dataset(\"squad\", split=\"train\")\n",
    "squad_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_squad = squad_ds.flatten()\n",
    "flat_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(example):\n",
    "    example[\"answers.text\"] = 'My Answer: ' + example[\"answers.text\"][0]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 253/87599 [00:00<00:37, 2317.97 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 87599/87599 [00:19<00:00, 4580.93 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['My Answer: Saint Bernadette Soubirous',\n",
       " 'My Answer: a copper statue of Christ',\n",
       " 'My Answer: the Main Building',\n",
       " 'My Answer: a Marian place of prayer and reflection',\n",
       " 'My Answer: a golden statue of the Virgin Mary']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = flat_squad.map(add_prefix)\n",
    "updated_dataset[\"answers.text\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 233/87599 [00:00<00:39, 2223.23 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 87599/87599 [00:20<00:00, 4352.45 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'title',\n",
       " 'context',\n",
       " 'question',\n",
       " 'answers.answer_start',\n",
       " 'updated_answer']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single process\n",
    "updated_dataset = flat_squad.map(lambda example: {\"updated_answer\": example[\"answers.text\"]}, remove_columns=[\"answers.text\"])\n",
    "updated_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 87599/87599 [00:23<00:00, 3696.08 examples/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'title',\n",
       " 'context',\n",
       " 'question',\n",
       " 'answers.answer_start',\n",
       " 'updated_answer']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiprocess\n",
    "updated_dataset = flat_squad.map(lambda example: {\"updated_answer\": example[\"answers.text\"]},\n",
    "                                 remove_columns=[\"answers.text\"],\n",
    "                                 num_proc=4)\n",
    "updated_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_squad[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_examples(examples):\n",
    "    # print(examples['question'])\n",
    "    chunks = []\n",
    "    for sentence in examples[\"question\"]:\n",
    "        print(sentence)\n",
    "        chunks += [sentence[i:i + 10] for i in range(0, len(sentence),10)]\n",
    "    return {\"chunked_questions\": chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_question_length(examples):\n",
    "    return {\"question_length\": [len(example[0][\"question\"].split()) for example in examples]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is in\n",
       "</pre>\n"
      ],
      "text/plain": [
       "What is in\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> front of \n",
       "</pre>\n"
      ],
      "text/plain": [
       " front of \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">the Notre \n",
       "</pre>\n"
      ],
      "text/plain": [
       "the Notre \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Dame Main \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Dame Main \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Building?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Building?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = \"What is in front of the Notre Dame Main Building?\"\n",
    "chunb = []\n",
    "for i in range(0, len(sentence), 10):\n",
    "    print(sentence[i: i + 10])\n",
    "    chunb.append(sentence[i: i + 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is in', ' front of ', 'the Notre ', 'Dame Main ', 'Building?']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkd_batches = squad_ds.map(chunk_examples, batched=True, batch_size=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from transformers import pipeline\n",
    "\n",
    "fillmask = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
    "\n",
    "mask_token = fillmask.tokenizer.mask_token\n",
    "\n",
    "smaller_dataset = dataset.filter(lambda e, i: i<100, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(examples):\n",
    "    outputs = []\n",
    "    for sentence in examples[\"sentence1\"]:\n",
    "        words = sentence.split(' ')\n",
    "        K = randint(1, len(words)-1)\n",
    "        masked_sentence = \" \".join(words[:K]  + [mask_token] + words[K+1:])\n",
    "        predictions = fillmask(masked_sentence)\n",
    "        augmented_sequences = [predictions[i][\"sequence\"] for i in range(3)]\n",
    "        outputs += [sentence] + augmented_sequences\n",
    "    return {\"data\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = smaller_dataset.map(augment_data,\n",
    "                                        batched=True,\n",
    "                                        remove_columns=dataset.column_names,\n",
    "                                        batch_size=8)\n",
    "augmented_dataset[:9][\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "\n",
    "wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "assert bookcorpus.features.type == wiki.features.type\n",
    "\n",
    "bert_dataset = concatenate_datasets([bookcorpus, wiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import interleave_datasets\n",
    "\n",
    "seed = 42\n",
    "probabilities = [0.3, 0.5, 0.2]\n",
    "d1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n",
    "d2 = Dataset.from_dict({\"a\": [10, 11, 12, 13]})\n",
    "d3 = Dataset.from_dict({\"a\": [20, 21, 22]})\n",
    "dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)\n",
    "dataset[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
