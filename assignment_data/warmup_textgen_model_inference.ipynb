{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rich transformers torch accelerate bitsandbytes peft"
      ],
      "metadata": {
        "id": "Pix8elC8JEVL"
      },
      "id": "Pix8elC8JEVL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "301491de",
      "metadata": {
        "id": "301491de"
      },
      "source": [
        "Task: Text Generation\n",
        "\n",
        "Each task has its own default model in the pipeline.\n",
        "\n",
        "- Causal Language Modeling\n",
        "\n",
        "- Masked Language Modeling\n",
        "\n",
        "Another type of variation is\n",
        "\n",
        "- Text Generation\n",
        "\n",
        "- Text-to-Text Generation models\n",
        "\n",
        "\n",
        "Variety of LMs in HuggingFace\n",
        "\n",
        "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
        "\n",
        "\n",
        "Some of the Text generation Tasks\n",
        "\n",
        "Code Generation: Trained to generate code\n",
        "\n",
        "https://huggingface.co/spaces/bigcode/bigcode-playground\n",
        "\n",
        "Instruction Model: Those that are trained on instruction\n",
        "\n",
        "Stories generation: A prompt starts the Stories Generation\n",
        "\n",
        "\n",
        "Quantization Using BitsAndBytes is touched\n",
        "\n",
        "- Grokking how the models are shrunk and loaded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "280bdde1-8562-4069-968f-f022fffda98c",
      "metadata": {
        "id": "280bdde1-8562-4069-968f-f022fffda98c",
        "outputId": "3c0a490c-d570-4a30-b430-bcba3ea3a907"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# there is default chat for the model, so it must give output\n",
        "roberta_tokenizer.default_chat_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c972a40-d284-4214-b37b-14ad0e1eda0d",
      "metadata": {
        "id": "4c972a40-d284-4214-b37b-14ad0e1eda0d"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# get the questions\n",
        "questions = [q for q in dataset['instruction']]\n",
        "\n",
        "random.shuffle(questions)  # no need to reassign\n",
        "\n",
        "prompt_template = \"\"\"Below is a riddle. Come up with 10 more.\n",
        "Output just the riddles. No numbering and don't output anything else\"\"\"\n",
        "\n",
        "simple_request = \"Generate 10 riddles that you know\"\n",
        "\n",
        "prompt_riddles = prompt_template + \"\\n\\n\".join(questions[0:10])\n",
        "\n",
        "# need to use join, list and string dont concatenate\n",
        "\n",
        "messages_to_model = [{\"role\":\"user\", \"content\":prompt_riddles}]\n",
        "\n",
        "simple_message = [{\"role\":\"user\", \"content\":simple_request}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dff43e9-10d7-47ea-8384-a8d6c3ae95bc",
      "metadata": {
        "id": "1dff43e9-10d7-47ea-8384-a8d6c3ae95bc",
        "outputId": "31a28bfc-3eb4-420f-a2d3-3f57e14e35a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[41552, 15483,   757,  1215, 13124, 15483, 15698, 12105, 50118, 40025,\n",
              "           877,   158,   910, 40741,    14,    47,   216, 41552, 15483,   757,\n",
              "          1215,  1397, 15483, 15698, 50118]], device='cuda:0')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# roberta_input = roberta_tokenizer.apply_chat_template(messages_to_roberta, return_tensors='pt').to('cuda')\n",
        "roberta_input = roberta_tokenizer.apply_chat_template(simple_message,\n",
        "                                                      return_tensors='pt').to('cuda')\n",
        "roberta_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d76cd3-c2da-4ffe-8975-bbdf197f5d98",
      "metadata": {
        "id": "e2d76cd3-c2da-4ffe-8975-bbdf197f5d98",
        "outputId": "b2bdf18f-2eb9-41db-c337-198ad45e0008"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|im_start|>user\\nGenerate 10 riddles that you know<|im_end|>\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "roberta_tokenizer.decode(roberta_input[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708f972a-4ac4-4b85-a1aa-a0c2579df219",
      "metadata": {
        "id": "708f972a-4ac4-4b85-a1aa-a0c2579df219",
        "outputId": "af59b289-182f-4267-c4b2-df81fc802f42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"bos_token_id\": 0,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"pad_token_id\": 1\n",
              "}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "roberta.generation_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4dee59a-9963-43f1-80ce-2e084b5a62e2",
      "metadata": {
        "id": "b4dee59a-9963-43f1-80ce-2e084b5a62e2"
      },
      "outputs": [],
      "source": [
        "roberta_output = roberta.generate(\n",
        "    roberta_input,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=True,\n",
        "    pad_token_id=roberta_tokenizer.eos_token_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b0ee9b6-54ca-4b5f-aa8d-0bca5ca97bb2",
      "metadata": {
        "id": "5b0ee9b6-54ca-4b5f-aa8d-0bca5ca97bb2",
        "outputId": "3412416c-efd4-4b0d-a202-a80358d8df34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([2], device='cuda:0')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = roberta_output[0][len(roberta_input[0]):]\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73ccce0e-f9fe-4497-87de-ea684957a9ca",
      "metadata": {
        "id": "73ccce0e-f9fe-4497-87de-ea684957a9ca",
        "outputId": "5dd488d9-f1d7-48d7-a4ee-3a017eb4496c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_text = roberta_tokenizer.decode(output, skip_special_tokens=True)\n",
        "output_text # There is no output at all. checking why, with a simpler prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f34b7a99-4491-4020-8928-699714745785",
      "metadata": {
        "id": "f34b7a99-4491-4020-8928-699714745785"
      },
      "outputs": [],
      "source": [
        "del roberta  # removes the memory handles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb16ef4-15b6-4458-9f88-0ac0d5153e7c",
      "metadata": {
        "id": "ddb16ef4-15b6-4458-9f88-0ac0d5153e7c"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()  # garbage collects the memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eabeee4-7eb6-4abf-a991-e45add40a5d2",
      "metadata": {
        "id": "0eabeee4-7eb6-4abf-a991-e45add40a5d2"
      },
      "source": [
        "Diving into BitsAndBytes Configuration.\n",
        "\n",
        "BitsAndBytesConfig(\n",
        "     load_in_8bit=False,\n",
        "  \n",
        "      load_in_4bit=False,\n",
        "\n",
        "       llm_int8_threshold=6.0,\n",
        "\n",
        "        llm_int8_skip_modules=None,\n",
        "    \n",
        "    llm_int8_enable_fp32_cpu_offload=False,\n",
        "    \n",
        "    llm_int8_has_fp16_weight=False\n",
        "    ,\n",
        "    bnb_4bit_compute_dtype=Non\n",
        "    e,\n",
        "    bnb_4bit_quant_type='fp\n",
        "    4',\n",
        "    bnb_4bit_use_double_quant=Fa\n",
        "    lse,\n",
        "    **kwargs,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e62d82c9-84df-4312-a32b-a924a4adbd86",
      "metadata": {
        "id": "e62d82c9-84df-4312-a32b-a924a4adbd86"
      },
      "outputs": [],
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b20e92f7-84f7-4f10-8d0e-e5a1ae5eef95",
      "metadata": {
        "id": "b20e92f7-84f7-4f10-8d0e-e5a1ae5eef95"
      },
      "source": [
        "#### Working on llama2-7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ae303e-50c9-4f81-a655-cc0653007b7d",
      "metadata": {
        "id": "d8ae303e-50c9-4f81-a655-cc0653007b7d",
        "outputId": "cfff9f43-78aa-45fa-d883-c11c77ccc53a",
        "colab": {
          "referenced_widgets": [
            "ceb2dbd733ba424babd2a14741a2e82c"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ceb2dbd733ba424babd2a14741a2e82c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# llama_path = \"/home/kamal/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93/\"\n",
        "\n",
        "gemma_path = \"google/gemma-2b-it\"\n",
        "gemma = AutoModelForCausalLM.from_pretrained(\n",
        "    # pretrained_model_name_or_path='/home/aicoder/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/718cb189da9c5b2e55abe86f2eeffee9b4ae0dad/\n",
        "    gemma_path,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    local_files_only=True\n",
        ") # takes 11GB of VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c37fe38-1b2c-4983-ae46-032c66467788",
      "metadata": {
        "scrolled": true,
        "id": "5c37fe38-1b2c-4983-ae46-032c66467788",
        "outputId": "859fbfd2-c3fd-4e53-fc09-60758e265a5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\""
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gemma_tokenizer = AutoTokenizer.from_pretrained(gemma_path)\n",
        "# print(llama_tokenizer.default_chat_template) # rich's print fails due to tag\n",
        "gemma_tokenizer.default_chat_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac90ad4-ab13-4595-857b-8b982f4a6ae5",
      "metadata": {
        "id": "7ac90ad4-ab13-4595-857b-8b982f4a6ae5",
        "outputId": "4b2075f1-53e8-4d0b-9479-f3dddc2b5562"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bos_token': '<bos>',\n",
              " 'eos_token': '<eos>',\n",
              " 'unk_token': '<unk>',\n",
              " 'pad_token': '<pad>',\n",
              " 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gemma_tokenizer.special_tokens_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a601624-37a0-43db-868a-c2db958d2758",
      "metadata": {
        "id": "9a601624-37a0-43db-868a-c2db958d2758",
        "outputId": "3cfa0181-177e-45e5-bf67-d18bd550ca3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[     2,    106,   1645,    108,  38557, 235248, 235274, 235276, 193130,\n",
              "            674,    692,   1230,    107,    108]], device='cuda:0')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gemma_input = llama_tokenizer.apply_chat_template(simple_message,\n",
        "                                                  return_tensors='pt').to('cuda')\n",
        "gemma_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35cb9f83-70a5-4103-939f-9897bc82ecbf",
      "metadata": {
        "id": "35cb9f83-70a5-4103-939f-9897bc82ecbf"
      },
      "outputs": [],
      "source": [
        "gemma_output = gemma.generate(\n",
        "    gemma_input,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    pad_token_id = llama_tokenizer.eos_token_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "937d4ebe-f79f-4459-9628-ea62468772ea",
      "metadata": {
        "id": "937d4ebe-f79f-4459-9628-ea62468772ea",
        "outputId": "2b4ce79f-2106-4200-c027-643216301c4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'user\\nGenerate 10 riddles that you know\\n1. I have keys, but no doors.\\nI have space, but no room.\\nI have pages, but no words.\\n\\nWhat am I?\\n\\n\\n2. I have a bed, but I never sleep.\\nI have a mouth, but I never speak.\\nI have ears, but I never hear.\\n\\nWhat am I?\\n\\n\\n3. I have a tongue, but I never eat.\\nI have a head, but I never wear.\\nI have a'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# llama_output = llama_output[len(llama_input[0]):]\n",
        "# llama_output\n",
        "output = gemma_tokenizer.decode(gemma_output[0],skip_special_tokens=True)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "297e7798-23ef-4dea-a1ca-e4abeef329c7",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true,
        "id": "297e7798-23ef-4dea-a1ca-e4abeef329c7",
        "outputId": "759a0fd8-f864-4173-bc12-135ea5caffc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[     2,    106,   1645,    108,  33501,    603,    476, 133326, 235265,\n",
              "          12542,    908,    675, 235248, 235274, 235276,    978, 235265,    108,\n",
              "           6140,   1317,    573, 193130, 235265,   1307,  96085,    578,   1453,\n",
              "         235303, 235251,   5033,   4341,   1354, 235285,  25469,    578,  10084,\n",
              "            578,  47331,   2003,    575,    861,   3142, 235265,    590,   1144,\n",
              "            793,  12100, 235269,    578,    590,   1453, 235303, 235251,   8044,\n",
              "          24306, 235265,    109,   1969,   2174,  11807,  28294, 235269,    665,\n",
              "            603,  24048,   1154,    476,  75735, 235265,   1165,    603,  10545,\n",
              "            675,    573,  25023, 235269,    578,    573,  25023,    603,  14471,\n",
              "         235265,    109, 159960, 235267,    685,    476,  12425,    575,    573,\n",
              "           5455,    576,   3354, 235269,  82056,    901,  90892,   1013,   2764,\n",
              "            476,  26911, 235265,  13227,  14987,    901,   3695,   8829, 235265,\n",
              "           2625,    970,   7888,   5604,  25201, 235269,    901,    674, 235303,\n",
              "         235256,   1861,    692,   2447,   1230,    682,    696,    832, 235265,\n",
              "            109, 235285,   1144,    476,   3967, 235303, 235256,   1963,   4034,\n",
              "         235265,   3194,   6181, 235269,    970,   2971,  49992,    675,   6785,\n",
              "         235265,   3194,  22121,    577,  11342, 235269,   1593,    970,  16482,\n",
              "            590,  10872, 235265,   5040,    692,    798,   4234,    573, 200967,\n",
              "          19312, 235265,    109, 235285,  66714,    611,    573,   6683, 235265,\n",
              "           1474,   9407,    611,    476,  61188, 235265,    109, 235280,   3741,\n",
              "           2346,  87980, 235269,  10240,    689,   2621, 235269,   3599,  13658,\n",
              "          29936,  13241,   2819, 235265, 235248,    109,   2169,    476,   3733,\n",
              "         235269,    590,   1144,   2145,   5985,    578,  10548, 235265,   2065,\n",
              "           1914,    682, 235269,    590,   3831,    476,   2040,    576,   5961,\n",
              "         235265,   2065,   1914,    682,   1653, 235269,    590,   1144,    573,\n",
              "           9670,    576,   5628, 235265,  76851,    682, 235269,    590,   3831,\n",
              "            573,  11988,    576,  59356, 235265,    109,   6571,    603,    693,\n",
              "            674,  10140,   2346,    476,   2590, 235265,   1474,    926,   3036,\n",
              "            611,    926,   1355, 235336,    109,   1841,    798,   6437,   4630,\n",
              "           3631,    578,   2001,   1174,    476,   1913,   1069, 235336,    109,\n",
              "           1841,    919, 235248, 235274, 235304,  16346,    901,    793,   1156,\n",
              "          29998, 235336,    107,    108]], device='cuda:0')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# working on getting the riddle based on the 10 riddles input\n",
        "gemma_10_input = gemma_tokenizer.apply_chat_template(messages_to_model,\n",
        "                                                     return_tensors='pt').to('cuda')\n",
        "gemma_10_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c93b98fc-2031-4d23-a5f5-1ee4c4d8ccec",
      "metadata": {
        "id": "c93b98fc-2031-4d23-a5f5-1ee4c4d8ccec",
        "outputId": "74baddf5-ba64-4ac4-8fb3-6a2bdf32fe5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"<bos><start_of_turn>user\\nBelow is a riddle. Come up with 10 more.\\nOutput just the riddles. No numbering and don't output anything elseI bubble and laugh and spit water in your face. I am no lady, and I don't wear lace.\\n\\nAn open ended barrel, it is shaped like a hive. It is filled with the flesh, and the flesh is alive.\\n\\nStealthy as a shadow in the dead of night, cunning but affectionate if given a bite. Never owned but often loved. At my sport considered cruel, but that's because you never know me at all.\\n\\nI am a fire's best friend. When fat, my body fills with wind. When pushed to thin, through my nose I blow. Then you can watch the embers glow.\\n\\nI crawl on the earth. And rise on a pillar.\\n\\nA box without hinges, lock or key, yet golden treasure lies within. \\n\\nAs a whole, I am both safe and secure. Behead me, I become a place of meeting. Behead me again, I am the partner of ready. Restore me, I become the domain of beasts.\\n\\nWho is he that runs without a leg. And his house on his back?\\n\\nWhat can touch someone once and last them a life time?\\n\\nWhat has 13 hearts but no other organs?<end_of_turn>\\n\""
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gemma_tokenizer.decode(llama_10_input[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b73d02a-4ca6-48d4-a5f3-2ce69a0fe673",
      "metadata": {
        "id": "4b73d02a-4ca6-48d4-a5f3-2ce69a0fe673"
      },
      "outputs": [],
      "source": [
        "gemma_10_output = gemma.generate(\n",
        "    llama_10_input,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=True,\n",
        "    pad_token_id=llama_tokenizer.eos_token_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f82462-d93c-4642-828c-7eca3cd2c0cc",
      "metadata": {
        "id": "f6f82462-d93c-4642-828c-7eca3cd2c0cc",
        "outputId": "9e48e974-2aeb-4878-b040-349318f86eaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"user\\nBelow is a riddle. Come up with 10 more.\\nOutput just the riddles. No numbering and don't output anything elseI bubble and laugh and spit water in your face. I am no lady, and I don't wear lace.\\n\\nAn open ended barrel, it is shaped like a hive. It is filled with the flesh, and the flesh is alive.\\n\\nStealthy as a shadow in the dead of night, cunning but affectionate if given a bite. Never owned but often loved. At my sport considered cruel, but that's because you never know me at all.\\n\\nI am a fire's best friend. When fat, my body fills with wind. When pushed to thin, through my nose I blow. Then you can watch the embers glow.\\n\\nI crawl on the earth. And rise on a pillar.\\n\\nA box without hinges, lock or key, yet golden treasure lies within. \\n\\nAs a whole, I am both safe and secure. Behead me, I become a place of meeting. Behead me again, I am the partner of ready. Restore me, I become the domain of beasts.\\n\\nWho is he that runs without a leg. And his house on his back?\\n\\nWhat can touch someone once and last them a life time?\\n\\nWhat has 13 hearts but no other organs?\\nThe solution is a spider.\\n\\nHere are 10 riddles that follow the same theme:\\n\\n1. I have a nest, but I'm not a bird. I have silk, but I'm not a moth. I have wings, but I'm not an airplane. I hop, but I'm not a rabbit. I spin, but I'm not a spider. What am I?\\n\\n\\n2. I have a heart, but I'm not a human. I have a mouth, but I'm not a beast. I have a body, but I'm not a creature. I have a web, but I'm not a spider. What am I?\\n\\n\\n3. I have a mouth, but I'm not a human. I have a neck, but I'm not a beast. I have a body, but I'm not a creature. I have a shell, but I'm not a mollusc. What am I?\\n\\n\\n4. I have many eyes, but I'm not a creature. I have hairs, but I'm not a bird. I have a tail, but I'm not a reptile. What am I?\\n\\n\\n5. I have a nest, but I'm not a bird. I have silk, but I'm not a moth. I have wings, but I'm not an airplane. I hop, but I'm not a rabbit. I spin, but I'm not a spider. What am I?\\n\\n\\n6. I have a heart, but I'm not a human. I have a mouth, but I'm not a beast. I have a body, but I'm not a creature. I have a beard, but I'm not a man. What am I?\\n\\n\\n7. I have a mouth, but I'm not a human. I have a neck, but I'm not a beast. I have a body, but I'm not a creature. I have a beard, but I'm not a man. What am I?\\n\\n\\n8. I have a nest, but I'm not a bird. I have silk, but I'm not a moth. I have wings, but I'm not an airplane. I hop, but I'm not a rabbit. I spin, but I'm not a spider. What am I?\\n\\n\\n\""
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_10 = gemma_tokenizer.decode(gemma_10_output[0], skip_special_tokens=True)\n",
        "output_10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a26e28d3-a303-47f3-9935-e56258002262",
      "metadata": {
        "id": "a26e28d3-a303-47f3-9935-e56258002262"
      },
      "outputs": [],
      "source": [
        "del gemma\n",
        "torch.cuda.empty_cache()  # this time the model is not offloadin\n",
        "# restarting the server to release the memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34081f37-c363-40e6-aa75-8b5ab80a9fb2",
      "metadata": {
        "id": "34081f37-c363-40e6-aa75-8b5ab80a9fb2"
      },
      "source": [
        "### Llama observation\n",
        "\n",
        "- Llama provides the output of 10 riddles\n",
        "\n",
        "- Llama encoding and decoding is working same as roberta models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00d76358-9826-424a-97e6-f4ef06258f27",
      "metadata": {
        "id": "00d76358-9826-424a-97e6-f4ef06258f27",
        "outputId": "83568689-a175-4696-a590-e7ab5c6991ee",
        "colab": {
          "referenced_widgets": [
            "aa7f9689dc15421895cfb4b75e85a743"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa7f9689dc15421895cfb4b75e85a743",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "code_llama_path = \"/home/kamal/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/bc5283229e2fe411552f55c71657e97edf79066c/\"\n",
        "code_tokenizer = AutoTokenizer.from_pretrained(code_llama_path)\n",
        "codellama = AutoModelForCausalLM.from_pretrained(\n",
        "    code_llama_path,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")  # takes around 11.5GB of VRAM\n",
        "# with 4-bit quantization 5GB of VRAM is consumed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d473eea-06c7-4ade-852b-51f32e5292a0",
      "metadata": {
        "id": "1d473eea-06c7-4ade-852b-51f32e5292a0",
        "outputId": "b433cda7-357f-4b99-ecdb-52e670064ef3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\\\n\\\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\\\'t know the answer to a question, please don\\\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\\\n' + content.strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\""
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "code_tokenizer.default_chat_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d44f29-f0d5-4a9d-8ce5-98f97cd071e5",
      "metadata": {
        "id": "82d44f29-f0d5-4a9d-8ce5-98f97cd071e5"
      },
      "outputs": [],
      "source": [
        "simple_request = \"Generate 10 riddles that you know\"\n",
        "\n",
        "simple_message = [{\"role\":\"user\", \"content\":simple_request}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4b07afe-d80b-454d-8c15-e21177e3b3fa",
      "metadata": {
        "id": "e4b07afe-d80b-454d-8c15-e21177e3b3fa"
      },
      "outputs": [],
      "source": [
        "code_request = \"Generate 10 dictionary related problems\"\n",
        "\n",
        "code_message = [{\"role\":\"user\", \"content\":code_request}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7942f2ed-26bb-4177-a280-92f3654d41f9",
      "metadata": {
        "id": "7942f2ed-26bb-4177-a280-92f3654d41f9",
        "outputId": "a09f5c8a-6c0f-4116-d71c-c33552bd0c63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using sep_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using cls_token, but it is not set yet.\n",
            "Using mask_token, but it is not set yet.\n"
          ]
        }
      ],
      "source": [
        "code_input = code_tokenizer.apply_chat_template(simple_message,return_tensors='pt').to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c4182ad-3d8b-4545-baf2-39b1d7ed1d2f",
      "metadata": {
        "id": "1c4182ad-3d8b-4545-baf2-39b1d7ed1d2f",
        "outputId": "578a0f64-977c-4cd6-f706-90f4bd2a08eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using sep_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using cls_token, but it is not set yet.\n",
            "Using mask_token, but it is not set yet.\n"
          ]
        }
      ],
      "source": [
        "code_input = code_tokenizer.apply_chat_template(code_message,return_tensors='pt').to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "541e2479-330a-4cfe-814f-b3703cd6a12d",
      "metadata": {
        "scrolled": true,
        "id": "541e2479-330a-4cfe-814f-b3703cd6a12d"
      },
      "outputs": [],
      "source": [
        "code_output = codellama.generate(\n",
        "    code_input,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=True,\n",
        "    pad_token_id=code_tokenizer.eos_token_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09804125-3a3c-4876-b81f-7ae4ca372c5d",
      "metadata": {
        "scrolled": true,
        "id": "09804125-3a3c-4876-b81f-7ae4ca372c5d"
      },
      "outputs": [],
      "source": [
        "code_output = codellama.generate(\n",
        "    code_input,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=True,\n",
        "    pad_token_id=code_tokenizer.eos_token_id,\n",
        "    temperature=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4996cfd8-5120-4d7c-a69e-ed6455099d02",
      "metadata": {
        "id": "4996cfd8-5120-4d7c-a69e-ed6455099d02"
      },
      "outputs": [],
      "source": [
        "code_output = codellama.generate(\n",
        "    code_input,\n",
        "    max_new_tokens=500,\n",
        "    # do_sample=True,\n",
        "    pad_token_id=code_tokenizer.eos_token_id,\n",
        "    # temperature=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6318537-8343-4e1c-b218-25f04bd8c308",
      "metadata": {
        "id": "e6318537-8343-4e1c-b218-25f04bd8c308"
      },
      "outputs": [],
      "source": [
        "code_output = codellama.generate(\n",
        "    code_input,\n",
        "    max_new_tokens=500,\n",
        "    # do_sample=True,\n",
        "    pad_token_id=code_tokenizer.eos_token_id,\n",
        "    # temperature=0.2\n",
        "    repetition_penalty=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c754a9-97c9-4f93-9696-7da8d6a30f8e",
      "metadata": {
        "id": "e7c754a9-97c9-4f93-9696-7da8d6a30f8e"
      },
      "outputs": [],
      "source": [
        "code_output = codellama.generate(\n",
        "    code_input,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=True,\n",
        "    pad_token_id=code_tokenizer.eos_token_id,\n",
        "    # temperature=0.2\n",
        "    repetition_penalty=0.5,\n",
        "    top_p=10,\n",
        "    top_k=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90d42456-3166-40ad-99e3-2a83cd6aa3e7",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "90d42456-3166-40ad-99e3-2a83cd6aa3e7",
        "outputId": "4bab455a-e040-4cc9-f741-7d719258577b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaSSLayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaSSLayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaSSLayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaSSLayaayaSSLayaSSLayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaNaNayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLSSLSSLayaSSLayaayaSSLayaayaayaayaaya'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = code_output[0][len(code_input[0]):]\n",
        "code_tokenizer.decode(output, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82a3bfce-f1ae-4230-9a76-2c0746722370",
      "metadata": {
        "id": "82a3bfce-f1ae-4230-9a76-2c0746722370"
      },
      "source": [
        "### Code Llama observation\n",
        "\n",
        "- Model inference lead to OOM with 500 tokens request, when loaded with bfloat16\n",
        "\n",
        "- Model inference worked with 500 Tokens, with quant_config done with 4-bit.\n",
        "\n",
        "- In quantisation 1GB of Vram is consumed for inference\n",
        "\n",
        "\n",
        "- **The model out was gibberish**\n",
        "\n",
        "- Requested for the code related to dictionary\n",
        "\n",
        "- Even after reviewing the generation configs, only gibberish was generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61899424-60f5-40bb-ac35-d553dcd01869",
      "metadata": {
        "id": "61899424-60f5-40bb-ac35-d553dcd01869"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "876e49f2-7b0c-4f65-b4e0-a4961036f588",
      "metadata": {
        "id": "876e49f2-7b0c-4f65-b4e0-a4961036f588"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c13522fe-47b0-4d4e-ad0e-c657161bd643",
      "metadata": {
        "id": "c13522fe-47b0-4d4e-ad0e-c657161bd643"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "925d4392-fe66-4331-9a4b-c7cc39424ee9",
      "metadata": {
        "id": "925d4392-fe66-4331-9a4b-c7cc39424ee9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}