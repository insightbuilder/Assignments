{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "301602d4-a0df-407e-afa1-ccfdff046332",
   "metadata": {},
   "source": [
    "Notebook introduces the Embeddings and their use cases \n",
    "\n",
    "It all starts with Data, in this case the data is taken from the https://faq.ssa.gov/en-US/\n",
    "\n",
    "Note, Embedding is a process of converting a word or a number into a vector of certain dimensions\n",
    "Tokenizer and Embedding models are not same. They are different. \n",
    "\n",
    "Tokenizers are functions written in python that take a corpus of data and returns a dictionary-id map. Based on which the tokenizers, work on the sentences.\n",
    "\n",
    "Embedding models are Neural Networks coded in Torch/TF/Jax/Flax that are used for creating vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d81148-d9f3-48a0-b185-d37a0943e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to work with sentence transformers library. \n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_embedding = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2d8cc7-e5d2-4804-901d-669358cd58ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"How do I get a replacement Medicare card?\",\n",
    "        \"What is the monthly premium for Medicare Part B?\",\n",
    "        \"How do I terminate my Medicare Part B (medical insurance)?\",\n",
    "        \"How do I sign up for Medicare?\",\n",
    "        \"Can I sign up for Medicare Part B if I am working and have health insurance through an employer?\",\n",
    "        \"How do I sign up for Medicare Part B if I already have Part A?\",\n",
    "        \"What are Medicare late enrollment penalties?\",\n",
    "        \"What is Medicare and who can get it?\",\n",
    "        \"How can I get help with my Medicare Part A and Part B premiums?\",\n",
    "        \"What are the different parts of Medicare?\",\n",
    "        \"Will my Medicare premiums be higher because of my higher income?\",\n",
    "        \"What is TRICARE ?\",\n",
    "        \"Should I sign up for Medicare Part B if I have Veterans' Benefits?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01d477f-5ac3-4d1e-97ba-0a28a6f705e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_embedding.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "650429f5-8fc3-4359-9bcb-2439be375d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tokenized = model_embedding.tokenize(texts=texts[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60512292-b734-4458-aa7b-3f59b3b01ee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1044,  102],\n",
       "         [ 101, 1051,  102],\n",
       "         [ 101, 1059,  102],\n",
       "         [ 101,  102,    0],\n",
       "         [ 101, 1040,  102],\n",
       "         [ 101, 1051,  102],\n",
       "         [ 101,  102,    0],\n",
       "         [ 101, 1045,  102],\n",
       "         [ 101,  102,    0],\n",
       "         [ 101, 1043,  102],\n",
       "         [ 101, 1041,  102],\n",
       "         [ 101, 1056,  102],\n",
       "         [ 101,  102,    0],\n",
       "         [ 101, 1037,  102],\n",
       "         [ 101,  102,    0],\n",
       "         [ 101, 1054,  102],\n",
       "         [ 101, 1041,  102],\n",
       "         [ 101, 1052,  102],\n",
       "         [ 101, 1048,  102],\n",
       "         [ 101, 1037,  102],\n",
       "         [ 101, 1039,  102],\n",
       "         [ 101, 1041,  102],\n",
       "         [ 101, 1049,  102],\n",
       "         [ 101, 1041,  102],\n",
       "         [ 101, 1050,  102],\n",
       "         [ 101, 1056,  102],\n",
       "         [ 101,  102,    0],\n",
       "         [ 101, 1049,  102],\n",
       "         [ 101, 1041,  102],\n",
       "         [ 101, 1040,  102],\n",
       "         [ 101, 1045,  102],\n",
       "         [ 101, 1039,  102],\n",
       "         [ 101, 1037,  102],\n",
       "         [ 101, 1054,  102],\n",
       "         [ 101, 1041,  102],\n",
       "         [ 101,  102,    0],\n",
       "         [ 101, 1039,  102],\n",
       "         [ 101, 1037,  102],\n",
       "         [ 101, 1054,  102],\n",
       "         [ 101, 1040,  102],\n",
       "         [ 101, 1029,  102]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ebb98f5-8e9c-4158-ba7c-896aff4084f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_embed01 = model_embedding.encode(texts[0], convert_to_tensor=True)\n",
    "texts_embed01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfafe1a0-026f-423f-8926-ec98fdd6f712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_embed02 = model_embedding.encode(texts[1], convert_to_tensor=True)\n",
    "texts_embed02.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d2157bd-f14b-43bd-a66e-d8b85aadf557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import pytorch_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8d7d335-d0f5-4862-8ff3-bf577eefa451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4886]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = pytorch_cos_sim(texts_embed01, texts_embed02)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cecdf743-cbba-4256-a5b1-d839d3efb502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 384)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_texts = model_embedding.encode(texts)\n",
    "embedding_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66dbdff-e247-42fe-9531-3b66c68bc3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "embed_df = DataFrame(embedding_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "314fe9d4-969d-465b-b86c-8b9e71204262",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.023889</td>\n",
       "      <td>0.055259</td>\n",
       "      <td>-0.011655</td>\n",
       "      <td>-0.033414</td>\n",
       "      <td>-0.012261</td>\n",
       "      <td>-0.024873</td>\n",
       "      <td>-0.012663</td>\n",
       "      <td>0.025346</td>\n",
       "      <td>0.018508</td>\n",
       "      <td>-0.083508</td>\n",
       "      <td>-0.093020</td>\n",
       "      <td>0.014486</td>\n",
       "      <td>-0.017411</td>\n",
       "      <td>-0.088344</td>\n",
       "      <td>-0.004479</td>\n",
       "      <td>-0.046326</td>\n",
       "      <td>-0.013194</td>\n",
       "      <td>0.035382</td>\n",
       "      <td>0.062311</td>\n",
       "      <td>0.048590</td>\n",
       "      <td>-0.059118</td>\n",
       "      <td>0.054135</td>\n",
       "      <td>-0.064397</td>\n",
       "      <td>0.034024</td>\n",
       "      <td>0.006636</td>\n",
       "      <td>0.035917</td>\n",
       "      <td>-0.067838</td>\n",
       "      <td>-0.017735</td>\n",
       "      <td>-0.012722</td>\n",
       "      <td>0.046462</td>\n",
       "      <td>0.108644</td>\n",
       "      <td>0.023821</td>\n",
       "      <td>-0.026996</td>\n",
       "      <td>0.037174</td>\n",
       "      <td>0.097598</td>\n",
       "      <td>-0.027030</td>\n",
       "      <td>-0.045430</td>\n",
       "      <td>0.031817</td>\n",
       "      <td>-0.033746</td>\n",
       "      <td>-0.015198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045291</td>\n",
       "      <td>0.118322</td>\n",
       "      <td>0.054848</td>\n",
       "      <td>-0.040015</td>\n",
       "      <td>0.098105</td>\n",
       "      <td>0.022277</td>\n",
       "      <td>-0.030813</td>\n",
       "      <td>-0.005176</td>\n",
       "      <td>0.049103</td>\n",
       "      <td>0.045938</td>\n",
       "      <td>-0.023188</td>\n",
       "      <td>-0.027573</td>\n",
       "      <td>-0.040576</td>\n",
       "      <td>0.016116</td>\n",
       "      <td>0.025010</td>\n",
       "      <td>-0.058007</td>\n",
       "      <td>0.047965</td>\n",
       "      <td>0.117957</td>\n",
       "      <td>-0.008974</td>\n",
       "      <td>-0.013361</td>\n",
       "      <td>0.020989</td>\n",
       "      <td>-0.025200</td>\n",
       "      <td>-0.006896</td>\n",
       "      <td>-0.021131</td>\n",
       "      <td>0.005462</td>\n",
       "      <td>0.064137</td>\n",
       "      <td>0.026008</td>\n",
       "      <td>-0.029850</td>\n",
       "      <td>-0.011776</td>\n",
       "      <td>0.003090</td>\n",
       "      <td>-0.161688</td>\n",
       "      <td>-0.046426</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>-0.003342</td>\n",
       "      <td>0.027754</td>\n",
       "      <td>0.020411</td>\n",
       "      <td>0.005778</td>\n",
       "      <td>0.034098</td>\n",
       "      <td>-0.006889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.012688</td>\n",
       "      <td>0.046874</td>\n",
       "      <td>-0.010502</td>\n",
       "      <td>-0.020384</td>\n",
       "      <td>-0.013361</td>\n",
       "      <td>0.042322</td>\n",
       "      <td>0.016628</td>\n",
       "      <td>-0.004099</td>\n",
       "      <td>-0.002607</td>\n",
       "      <td>-0.010188</td>\n",
       "      <td>-0.044768</td>\n",
       "      <td>0.019365</td>\n",
       "      <td>0.031505</td>\n",
       "      <td>-0.118893</td>\n",
       "      <td>0.019850</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>0.034993</td>\n",
       "      <td>-0.083673</td>\n",
       "      <td>0.056933</td>\n",
       "      <td>0.057396</td>\n",
       "      <td>-0.057795</td>\n",
       "      <td>-0.005447</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>0.014473</td>\n",
       "      <td>0.146743</td>\n",
       "      <td>-0.053123</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.030637</td>\n",
       "      <td>0.055512</td>\n",
       "      <td>0.043963</td>\n",
       "      <td>0.047002</td>\n",
       "      <td>0.044337</td>\n",
       "      <td>0.020708</td>\n",
       "      <td>-0.004741</td>\n",
       "      <td>-0.008704</td>\n",
       "      <td>-0.039581</td>\n",
       "      <td>-0.063424</td>\n",
       "      <td>-0.011725</td>\n",
       "      <td>-0.090585</td>\n",
       "      <td>-0.045387</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063684</td>\n",
       "      <td>0.099501</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.042053</td>\n",
       "      <td>0.054385</td>\n",
       "      <td>-0.017293</td>\n",
       "      <td>-0.007450</td>\n",
       "      <td>0.034746</td>\n",
       "      <td>-0.000616</td>\n",
       "      <td>-0.050755</td>\n",
       "      <td>-0.040021</td>\n",
       "      <td>0.014303</td>\n",
       "      <td>0.025885</td>\n",
       "      <td>-0.062788</td>\n",
       "      <td>0.040704</td>\n",
       "      <td>-0.028741</td>\n",
       "      <td>0.069934</td>\n",
       "      <td>-0.024656</td>\n",
       "      <td>0.064530</td>\n",
       "      <td>0.014862</td>\n",
       "      <td>0.030004</td>\n",
       "      <td>-0.010374</td>\n",
       "      <td>-0.090460</td>\n",
       "      <td>-0.062121</td>\n",
       "      <td>-0.015130</td>\n",
       "      <td>-0.003932</td>\n",
       "      <td>0.075132</td>\n",
       "      <td>0.052699</td>\n",
       "      <td>0.020436</td>\n",
       "      <td>0.024714</td>\n",
       "      <td>-0.061594</td>\n",
       "      <td>-0.020717</td>\n",
       "      <td>-0.009082</td>\n",
       "      <td>-0.029260</td>\n",
       "      <td>-0.066253</td>\n",
       "      <td>0.065257</td>\n",
       "      <td>0.013229</td>\n",
       "      <td>-0.023103</td>\n",
       "      <td>-0.002785</td>\n",
       "      <td>0.010474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.119412</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>-0.092734</td>\n",
       "      <td>0.007773</td>\n",
       "      <td>-0.005325</td>\n",
       "      <td>0.034506</td>\n",
       "      <td>-0.051981</td>\n",
       "      <td>-0.006265</td>\n",
       "      <td>-0.006110</td>\n",
       "      <td>-0.079471</td>\n",
       "      <td>0.036207</td>\n",
       "      <td>-0.009710</td>\n",
       "      <td>-0.081195</td>\n",
       "      <td>-0.001876</td>\n",
       "      <td>-0.013249</td>\n",
       "      <td>-0.042756</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>-0.007266</td>\n",
       "      <td>0.100785</td>\n",
       "      <td>-0.002075</td>\n",
       "      <td>0.042169</td>\n",
       "      <td>-0.023942</td>\n",
       "      <td>0.098594</td>\n",
       "      <td>0.072433</td>\n",
       "      <td>-0.002734</td>\n",
       "      <td>0.016057</td>\n",
       "      <td>0.005720</td>\n",
       "      <td>-0.026609</td>\n",
       "      <td>-0.013365</td>\n",
       "      <td>0.097391</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>-0.016172</td>\n",
       "      <td>-0.003942</td>\n",
       "      <td>0.034441</td>\n",
       "      <td>-0.013009</td>\n",
       "      <td>-0.109540</td>\n",
       "      <td>-0.019242</td>\n",
       "      <td>-0.003607</td>\n",
       "      <td>-0.060187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015841</td>\n",
       "      <td>0.088835</td>\n",
       "      <td>-0.022281</td>\n",
       "      <td>0.007992</td>\n",
       "      <td>0.044760</td>\n",
       "      <td>-0.002664</td>\n",
       "      <td>-0.015018</td>\n",
       "      <td>-0.024615</td>\n",
       "      <td>0.043037</td>\n",
       "      <td>0.046402</td>\n",
       "      <td>-0.074185</td>\n",
       "      <td>0.007321</td>\n",
       "      <td>0.012401</td>\n",
       "      <td>-0.004225</td>\n",
       "      <td>0.040887</td>\n",
       "      <td>-0.013238</td>\n",
       "      <td>0.086007</td>\n",
       "      <td>0.130728</td>\n",
       "      <td>0.009953</td>\n",
       "      <td>0.053924</td>\n",
       "      <td>0.037271</td>\n",
       "      <td>-0.037933</td>\n",
       "      <td>-0.004120</td>\n",
       "      <td>-0.041604</td>\n",
       "      <td>-0.048431</td>\n",
       "      <td>0.110611</td>\n",
       "      <td>0.038085</td>\n",
       "      <td>-0.016102</td>\n",
       "      <td>-0.011424</td>\n",
       "      <td>-0.009410</td>\n",
       "      <td>-0.108326</td>\n",
       "      <td>-0.049646</td>\n",
       "      <td>-0.073399</td>\n",
       "      <td>-0.029898</td>\n",
       "      <td>-0.102734</td>\n",
       "      <td>0.062121</td>\n",
       "      <td>0.034605</td>\n",
       "      <td>0.016877</td>\n",
       "      <td>-0.023861</td>\n",
       "      <td>0.005264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.029711</td>\n",
       "      <td>0.023298</td>\n",
       "      <td>-0.057041</td>\n",
       "      <td>-0.012183</td>\n",
       "      <td>-0.013710</td>\n",
       "      <td>0.029796</td>\n",
       "      <td>0.063739</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>-0.045124</td>\n",
       "      <td>-0.040748</td>\n",
       "      <td>-0.131671</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.032849</td>\n",
       "      <td>-0.048718</td>\n",
       "      <td>-0.016917</td>\n",
       "      <td>-0.040010</td>\n",
       "      <td>-0.003435</td>\n",
       "      <td>-0.000405</td>\n",
       "      <td>0.049092</td>\n",
       "      <td>0.057811</td>\n",
       "      <td>0.007957</td>\n",
       "      <td>-0.014720</td>\n",
       "      <td>-0.055192</td>\n",
       "      <td>0.029432</td>\n",
       "      <td>0.086543</td>\n",
       "      <td>-0.034207</td>\n",
       "      <td>-0.004638</td>\n",
       "      <td>-0.006953</td>\n",
       "      <td>-0.017902</td>\n",
       "      <td>0.089433</td>\n",
       "      <td>0.138466</td>\n",
       "      <td>-0.004411</td>\n",
       "      <td>-0.012209</td>\n",
       "      <td>0.027505</td>\n",
       "      <td>0.056866</td>\n",
       "      <td>-0.016538</td>\n",
       "      <td>-0.030820</td>\n",
       "      <td>0.005954</td>\n",
       "      <td>-0.056146</td>\n",
       "      <td>-0.004276</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023350</td>\n",
       "      <td>0.117718</td>\n",
       "      <td>0.058016</td>\n",
       "      <td>0.007543</td>\n",
       "      <td>0.053195</td>\n",
       "      <td>0.029278</td>\n",
       "      <td>-0.005433</td>\n",
       "      <td>0.046559</td>\n",
       "      <td>-0.008911</td>\n",
       "      <td>-0.013223</td>\n",
       "      <td>-0.073022</td>\n",
       "      <td>-0.018384</td>\n",
       "      <td>-0.001908</td>\n",
       "      <td>-0.026813</td>\n",
       "      <td>0.075265</td>\n",
       "      <td>-0.090822</td>\n",
       "      <td>0.035911</td>\n",
       "      <td>0.121485</td>\n",
       "      <td>0.071004</td>\n",
       "      <td>-0.025873</td>\n",
       "      <td>-0.021903</td>\n",
       "      <td>0.062796</td>\n",
       "      <td>-0.012797</td>\n",
       "      <td>-0.006417</td>\n",
       "      <td>0.017931</td>\n",
       "      <td>0.035687</td>\n",
       "      <td>0.033231</td>\n",
       "      <td>0.021569</td>\n",
       "      <td>0.100695</td>\n",
       "      <td>-0.047331</td>\n",
       "      <td>-0.117682</td>\n",
       "      <td>0.031924</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>-0.020666</td>\n",
       "      <td>-0.005167</td>\n",
       "      <td>0.038370</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.033993</td>\n",
       "      <td>-0.010255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.025628</td>\n",
       "      <td>0.070389</td>\n",
       "      <td>-0.017380</td>\n",
       "      <td>-0.056567</td>\n",
       "      <td>0.028576</td>\n",
       "      <td>0.052823</td>\n",
       "      <td>0.067063</td>\n",
       "      <td>-0.052617</td>\n",
       "      <td>-0.054702</td>\n",
       "      <td>-0.116230</td>\n",
       "      <td>-0.126143</td>\n",
       "      <td>0.038227</td>\n",
       "      <td>0.011085</td>\n",
       "      <td>-0.027623</td>\n",
       "      <td>0.086316</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.038370</td>\n",
       "      <td>0.087459</td>\n",
       "      <td>-0.060004</td>\n",
       "      <td>0.007136</td>\n",
       "      <td>-0.052758</td>\n",
       "      <td>-0.003477</td>\n",
       "      <td>0.079192</td>\n",
       "      <td>-0.030614</td>\n",
       "      <td>0.034550</td>\n",
       "      <td>0.065704</td>\n",
       "      <td>-0.011732</td>\n",
       "      <td>0.051478</td>\n",
       "      <td>0.095803</td>\n",
       "      <td>-0.019129</td>\n",
       "      <td>-0.036677</td>\n",
       "      <td>0.015641</td>\n",
       "      <td>0.036194</td>\n",
       "      <td>-0.058811</td>\n",
       "      <td>-0.035086</td>\n",
       "      <td>0.022795</td>\n",
       "      <td>-0.081846</td>\n",
       "      <td>-0.027348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075405</td>\n",
       "      <td>0.129256</td>\n",
       "      <td>-0.058059</td>\n",
       "      <td>-0.019650</td>\n",
       "      <td>0.101450</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>-0.012665</td>\n",
       "      <td>0.038677</td>\n",
       "      <td>0.021085</td>\n",
       "      <td>-0.004969</td>\n",
       "      <td>-0.021644</td>\n",
       "      <td>-0.070017</td>\n",
       "      <td>0.060121</td>\n",
       "      <td>-0.107323</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>-0.093465</td>\n",
       "      <td>0.087102</td>\n",
       "      <td>0.094227</td>\n",
       "      <td>0.080545</td>\n",
       "      <td>0.032137</td>\n",
       "      <td>-0.011176</td>\n",
       "      <td>-0.064559</td>\n",
       "      <td>-0.031923</td>\n",
       "      <td>-0.051013</td>\n",
       "      <td>-0.017872</td>\n",
       "      <td>0.017034</td>\n",
       "      <td>0.061883</td>\n",
       "      <td>0.052157</td>\n",
       "      <td>0.101039</td>\n",
       "      <td>-0.056417</td>\n",
       "      <td>-0.118145</td>\n",
       "      <td>0.013343</td>\n",
       "      <td>-0.055188</td>\n",
       "      <td>-0.032723</td>\n",
       "      <td>0.008436</td>\n",
       "      <td>0.019169</td>\n",
       "      <td>0.048212</td>\n",
       "      <td>-0.040412</td>\n",
       "      <td>0.083346</td>\n",
       "      <td>0.026855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.022656</td>\n",
       "      <td>0.021160</td>\n",
       "      <td>0.005105</td>\n",
       "      <td>-0.046494</td>\n",
       "      <td>0.009074</td>\n",
       "      <td>0.041495</td>\n",
       "      <td>0.054268</td>\n",
       "      <td>-0.024185</td>\n",
       "      <td>-0.013483</td>\n",
       "      <td>-0.075966</td>\n",
       "      <td>-0.090702</td>\n",
       "      <td>-0.029076</td>\n",
       "      <td>0.045339</td>\n",
       "      <td>-0.077989</td>\n",
       "      <td>0.047003</td>\n",
       "      <td>-0.018830</td>\n",
       "      <td>-0.031521</td>\n",
       "      <td>-0.022798</td>\n",
       "      <td>0.021713</td>\n",
       "      <td>0.057836</td>\n",
       "      <td>-0.051639</td>\n",
       "      <td>-0.014933</td>\n",
       "      <td>-0.029978</td>\n",
       "      <td>0.023250</td>\n",
       "      <td>0.087391</td>\n",
       "      <td>-0.062931</td>\n",
       "      <td>-0.000420</td>\n",
       "      <td>0.062464</td>\n",
       "      <td>-0.021476</td>\n",
       "      <td>0.035335</td>\n",
       "      <td>0.125799</td>\n",
       "      <td>0.029123</td>\n",
       "      <td>-0.037065</td>\n",
       "      <td>0.013791</td>\n",
       "      <td>0.057291</td>\n",
       "      <td>-0.072491</td>\n",
       "      <td>-0.044007</td>\n",
       "      <td>0.026902</td>\n",
       "      <td>-0.039566</td>\n",
       "      <td>-0.066453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077669</td>\n",
       "      <td>0.099516</td>\n",
       "      <td>-0.011076</td>\n",
       "      <td>-0.007306</td>\n",
       "      <td>0.062561</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>-0.005897</td>\n",
       "      <td>0.007084</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.003088</td>\n",
       "      <td>-0.000738</td>\n",
       "      <td>-0.014339</td>\n",
       "      <td>-0.002310</td>\n",
       "      <td>-0.035318</td>\n",
       "      <td>0.033689</td>\n",
       "      <td>-0.050801</td>\n",
       "      <td>0.076678</td>\n",
       "      <td>0.099980</td>\n",
       "      <td>0.072010</td>\n",
       "      <td>0.044336</td>\n",
       "      <td>0.028311</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>-0.067214</td>\n",
       "      <td>-0.064206</td>\n",
       "      <td>-0.031583</td>\n",
       "      <td>0.060060</td>\n",
       "      <td>0.076265</td>\n",
       "      <td>0.012245</td>\n",
       "      <td>0.071965</td>\n",
       "      <td>-0.010519</td>\n",
       "      <td>-0.100110</td>\n",
       "      <td>0.010750</td>\n",
       "      <td>-0.031469</td>\n",
       "      <td>-0.004822</td>\n",
       "      <td>0.039657</td>\n",
       "      <td>0.026384</td>\n",
       "      <td>0.045514</td>\n",
       "      <td>0.059089</td>\n",
       "      <td>-0.017509</td>\n",
       "      <td>0.007166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.002911</td>\n",
       "      <td>0.060791</td>\n",
       "      <td>-0.009176</td>\n",
       "      <td>-0.006133</td>\n",
       "      <td>0.040493</td>\n",
       "      <td>0.036594</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>-0.031345</td>\n",
       "      <td>0.031806</td>\n",
       "      <td>-0.023495</td>\n",
       "      <td>0.071992</td>\n",
       "      <td>0.048723</td>\n",
       "      <td>0.081783</td>\n",
       "      <td>-0.050864</td>\n",
       "      <td>-0.005711</td>\n",
       "      <td>-0.080416</td>\n",
       "      <td>-0.012250</td>\n",
       "      <td>-0.003741</td>\n",
       "      <td>-0.029289</td>\n",
       "      <td>0.052237</td>\n",
       "      <td>-0.010236</td>\n",
       "      <td>0.037758</td>\n",
       "      <td>-0.079403</td>\n",
       "      <td>0.124539</td>\n",
       "      <td>0.091983</td>\n",
       "      <td>-0.010715</td>\n",
       "      <td>0.034181</td>\n",
       "      <td>-0.016364</td>\n",
       "      <td>-0.023802</td>\n",
       "      <td>0.015979</td>\n",
       "      <td>-0.060006</td>\n",
       "      <td>0.040025</td>\n",
       "      <td>-0.029828</td>\n",
       "      <td>0.017246</td>\n",
       "      <td>0.017604</td>\n",
       "      <td>-0.004945</td>\n",
       "      <td>-0.012642</td>\n",
       "      <td>0.005651</td>\n",
       "      <td>-0.064422</td>\n",
       "      <td>-0.001107</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037479</td>\n",
       "      <td>0.120514</td>\n",
       "      <td>0.092009</td>\n",
       "      <td>0.150646</td>\n",
       "      <td>0.059240</td>\n",
       "      <td>0.016865</td>\n",
       "      <td>-0.015192</td>\n",
       "      <td>0.032755</td>\n",
       "      <td>0.074319</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>-0.098705</td>\n",
       "      <td>-0.016977</td>\n",
       "      <td>-0.047840</td>\n",
       "      <td>-0.077831</td>\n",
       "      <td>0.031058</td>\n",
       "      <td>-0.023600</td>\n",
       "      <td>0.030114</td>\n",
       "      <td>-0.007999</td>\n",
       "      <td>0.037392</td>\n",
       "      <td>-0.022385</td>\n",
       "      <td>0.026635</td>\n",
       "      <td>-0.019759</td>\n",
       "      <td>-0.097564</td>\n",
       "      <td>0.022126</td>\n",
       "      <td>-0.026906</td>\n",
       "      <td>-0.008749</td>\n",
       "      <td>-0.033806</td>\n",
       "      <td>0.028241</td>\n",
       "      <td>-0.001251</td>\n",
       "      <td>-0.003584</td>\n",
       "      <td>-0.028763</td>\n",
       "      <td>-0.060458</td>\n",
       "      <td>-0.018598</td>\n",
       "      <td>-0.040189</td>\n",
       "      <td>-0.031486</td>\n",
       "      <td>-0.018299</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>-0.073420</td>\n",
       "      <td>0.016235</td>\n",
       "      <td>-0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.080526</td>\n",
       "      <td>0.059888</td>\n",
       "      <td>-0.048847</td>\n",
       "      <td>-0.040176</td>\n",
       "      <td>-0.063342</td>\n",
       "      <td>0.041848</td>\n",
       "      <td>0.119045</td>\n",
       "      <td>0.010652</td>\n",
       "      <td>-0.030095</td>\n",
       "      <td>-0.004561</td>\n",
       "      <td>-0.075150</td>\n",
       "      <td>0.081693</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>-0.084236</td>\n",
       "      <td>-0.061900</td>\n",
       "      <td>-0.021710</td>\n",
       "      <td>0.010616</td>\n",
       "      <td>-0.023371</td>\n",
       "      <td>0.030940</td>\n",
       "      <td>0.093385</td>\n",
       "      <td>-0.036370</td>\n",
       "      <td>0.042710</td>\n",
       "      <td>-0.061342</td>\n",
       "      <td>0.052395</td>\n",
       "      <td>0.041366</td>\n",
       "      <td>0.008109</td>\n",
       "      <td>-0.061988</td>\n",
       "      <td>-0.035993</td>\n",
       "      <td>-0.004243</td>\n",
       "      <td>0.071631</td>\n",
       "      <td>0.100317</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>0.049251</td>\n",
       "      <td>-0.039963</td>\n",
       "      <td>0.021823</td>\n",
       "      <td>-0.021824</td>\n",
       "      <td>0.033236</td>\n",
       "      <td>-0.022382</td>\n",
       "      <td>0.009573</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036822</td>\n",
       "      <td>0.103294</td>\n",
       "      <td>0.086007</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.036378</td>\n",
       "      <td>0.036222</td>\n",
       "      <td>-0.036158</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.004459</td>\n",
       "      <td>0.041679</td>\n",
       "      <td>-0.089641</td>\n",
       "      <td>-0.028039</td>\n",
       "      <td>0.027155</td>\n",
       "      <td>-0.080964</td>\n",
       "      <td>0.054563</td>\n",
       "      <td>-0.134662</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.086044</td>\n",
       "      <td>0.044157</td>\n",
       "      <td>0.023074</td>\n",
       "      <td>-0.026023</td>\n",
       "      <td>-0.024532</td>\n",
       "      <td>-0.021760</td>\n",
       "      <td>-0.052582</td>\n",
       "      <td>0.015607</td>\n",
       "      <td>0.022571</td>\n",
       "      <td>0.046028</td>\n",
       "      <td>0.050643</td>\n",
       "      <td>0.054423</td>\n",
       "      <td>-0.083213</td>\n",
       "      <td>-0.144566</td>\n",
       "      <td>0.020404</td>\n",
       "      <td>0.023088</td>\n",
       "      <td>0.005077</td>\n",
       "      <td>-0.055645</td>\n",
       "      <td>-0.007675</td>\n",
       "      <td>0.050791</td>\n",
       "      <td>-0.005989</td>\n",
       "      <td>0.134562</td>\n",
       "      <td>0.034817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.034388</td>\n",
       "      <td>0.072501</td>\n",
       "      <td>0.014440</td>\n",
       "      <td>-0.036695</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>0.063070</td>\n",
       "      <td>0.034683</td>\n",
       "      <td>-0.014531</td>\n",
       "      <td>-0.059862</td>\n",
       "      <td>-0.045383</td>\n",
       "      <td>-0.055213</td>\n",
       "      <td>-0.034528</td>\n",
       "      <td>0.009270</td>\n",
       "      <td>-0.095072</td>\n",
       "      <td>0.036745</td>\n",
       "      <td>0.025977</td>\n",
       "      <td>0.013696</td>\n",
       "      <td>0.004641</td>\n",
       "      <td>-0.044114</td>\n",
       "      <td>0.063383</td>\n",
       "      <td>-0.088903</td>\n",
       "      <td>0.013146</td>\n",
       "      <td>-0.037820</td>\n",
       "      <td>0.023436</td>\n",
       "      <td>0.079054</td>\n",
       "      <td>0.028170</td>\n",
       "      <td>-0.026840</td>\n",
       "      <td>0.012249</td>\n",
       "      <td>0.032541</td>\n",
       "      <td>-0.019416</td>\n",
       "      <td>0.079922</td>\n",
       "      <td>-0.043450</td>\n",
       "      <td>-0.048650</td>\n",
       "      <td>-0.006170</td>\n",
       "      <td>0.047211</td>\n",
       "      <td>-0.003600</td>\n",
       "      <td>-0.066540</td>\n",
       "      <td>0.031916</td>\n",
       "      <td>-0.052208</td>\n",
       "      <td>-0.048670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026549</td>\n",
       "      <td>0.120332</td>\n",
       "      <td>-0.020662</td>\n",
       "      <td>-0.007842</td>\n",
       "      <td>0.052714</td>\n",
       "      <td>0.005838</td>\n",
       "      <td>-0.021314</td>\n",
       "      <td>-0.019987</td>\n",
       "      <td>0.016647</td>\n",
       "      <td>-0.036486</td>\n",
       "      <td>-0.018713</td>\n",
       "      <td>0.007056</td>\n",
       "      <td>0.013114</td>\n",
       "      <td>-0.034846</td>\n",
       "      <td>0.019419</td>\n",
       "      <td>-0.048089</td>\n",
       "      <td>0.070016</td>\n",
       "      <td>0.015946</td>\n",
       "      <td>0.055659</td>\n",
       "      <td>0.041075</td>\n",
       "      <td>0.049812</td>\n",
       "      <td>-0.037412</td>\n",
       "      <td>-0.014560</td>\n",
       "      <td>-0.032269</td>\n",
       "      <td>-0.040533</td>\n",
       "      <td>0.043310</td>\n",
       "      <td>0.072315</td>\n",
       "      <td>0.006942</td>\n",
       "      <td>0.030646</td>\n",
       "      <td>0.013022</td>\n",
       "      <td>-0.114763</td>\n",
       "      <td>-0.035894</td>\n",
       "      <td>-0.019877</td>\n",
       "      <td>-0.033375</td>\n",
       "      <td>-0.030168</td>\n",
       "      <td>0.039412</td>\n",
       "      <td>0.044993</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>-0.025124</td>\n",
       "      <td>0.034191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.005964</td>\n",
       "      <td>0.025044</td>\n",
       "      <td>-0.003182</td>\n",
       "      <td>-0.025243</td>\n",
       "      <td>-0.039823</td>\n",
       "      <td>-0.012772</td>\n",
       "      <td>0.044713</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>-0.038213</td>\n",
       "      <td>-0.041149</td>\n",
       "      <td>-0.058540</td>\n",
       "      <td>0.070492</td>\n",
       "      <td>-0.029789</td>\n",
       "      <td>-0.046087</td>\n",
       "      <td>-0.016301</td>\n",
       "      <td>-0.080821</td>\n",
       "      <td>0.030458</td>\n",
       "      <td>-0.014638</td>\n",
       "      <td>0.012796</td>\n",
       "      <td>0.120223</td>\n",
       "      <td>-0.032289</td>\n",
       "      <td>0.035957</td>\n",
       "      <td>-0.018771</td>\n",
       "      <td>0.060870</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.037492</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.005595</td>\n",
       "      <td>-0.000582</td>\n",
       "      <td>-0.020706</td>\n",
       "      <td>0.063955</td>\n",
       "      <td>0.027098</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.017982</td>\n",
       "      <td>0.007558</td>\n",
       "      <td>0.045427</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.037546</td>\n",
       "      <td>-0.043077</td>\n",
       "      <td>-0.012915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012387</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.074588</td>\n",
       "      <td>0.018098</td>\n",
       "      <td>0.027723</td>\n",
       "      <td>0.073802</td>\n",
       "      <td>-0.010719</td>\n",
       "      <td>0.027924</td>\n",
       "      <td>0.027842</td>\n",
       "      <td>-0.001941</td>\n",
       "      <td>-0.052277</td>\n",
       "      <td>0.019475</td>\n",
       "      <td>0.042630</td>\n",
       "      <td>-0.044101</td>\n",
       "      <td>0.061573</td>\n",
       "      <td>-0.064164</td>\n",
       "      <td>0.077146</td>\n",
       "      <td>-0.030594</td>\n",
       "      <td>0.061598</td>\n",
       "      <td>0.050569</td>\n",
       "      <td>0.029921</td>\n",
       "      <td>-0.064050</td>\n",
       "      <td>-0.025672</td>\n",
       "      <td>0.022948</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>-0.004960</td>\n",
       "      <td>0.032083</td>\n",
       "      <td>0.061701</td>\n",
       "      <td>0.011159</td>\n",
       "      <td>-0.078794</td>\n",
       "      <td>-0.057621</td>\n",
       "      <td>0.021594</td>\n",
       "      <td>0.048983</td>\n",
       "      <td>-0.044541</td>\n",
       "      <td>-0.030137</td>\n",
       "      <td>0.006779</td>\n",
       "      <td>0.054854</td>\n",
       "      <td>0.029937</td>\n",
       "      <td>0.070214</td>\n",
       "      <td>0.041565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.039008</td>\n",
       "      <td>-0.010609</td>\n",
       "      <td>-0.007383</td>\n",
       "      <td>-0.050190</td>\n",
       "      <td>-0.002518</td>\n",
       "      <td>-0.041641</td>\n",
       "      <td>0.026969</td>\n",
       "      <td>-0.014801</td>\n",
       "      <td>-0.014127</td>\n",
       "      <td>-0.061637</td>\n",
       "      <td>-0.029511</td>\n",
       "      <td>0.053214</td>\n",
       "      <td>0.041974</td>\n",
       "      <td>-0.031195</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.023168</td>\n",
       "      <td>0.008201</td>\n",
       "      <td>0.028470</td>\n",
       "      <td>-0.022813</td>\n",
       "      <td>0.074679</td>\n",
       "      <td>0.005798</td>\n",
       "      <td>-0.038659</td>\n",
       "      <td>-0.035666</td>\n",
       "      <td>0.029840</td>\n",
       "      <td>0.145245</td>\n",
       "      <td>-0.014506</td>\n",
       "      <td>0.022171</td>\n",
       "      <td>0.037960</td>\n",
       "      <td>0.011715</td>\n",
       "      <td>0.051476</td>\n",
       "      <td>0.023953</td>\n",
       "      <td>-0.011509</td>\n",
       "      <td>0.034622</td>\n",
       "      <td>-0.014697</td>\n",
       "      <td>0.013907</td>\n",
       "      <td>-0.030273</td>\n",
       "      <td>-0.015182</td>\n",
       "      <td>0.007283</td>\n",
       "      <td>-0.060961</td>\n",
       "      <td>0.010886</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044096</td>\n",
       "      <td>0.174909</td>\n",
       "      <td>0.051677</td>\n",
       "      <td>-0.010071</td>\n",
       "      <td>-0.022709</td>\n",
       "      <td>0.052437</td>\n",
       "      <td>0.024236</td>\n",
       "      <td>-0.022785</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>-0.038633</td>\n",
       "      <td>0.005742</td>\n",
       "      <td>-0.046018</td>\n",
       "      <td>0.028250</td>\n",
       "      <td>-0.036483</td>\n",
       "      <td>0.078713</td>\n",
       "      <td>-0.036250</td>\n",
       "      <td>0.053055</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>-0.035608</td>\n",
       "      <td>-0.087633</td>\n",
       "      <td>0.029134</td>\n",
       "      <td>-0.031353</td>\n",
       "      <td>0.032814</td>\n",
       "      <td>-0.081044</td>\n",
       "      <td>0.039118</td>\n",
       "      <td>0.071065</td>\n",
       "      <td>0.036104</td>\n",
       "      <td>-0.116958</td>\n",
       "      <td>-0.098168</td>\n",
       "      <td>-0.031694</td>\n",
       "      <td>-0.052128</td>\n",
       "      <td>0.014774</td>\n",
       "      <td>-0.091150</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.053866</td>\n",
       "      <td>-0.083904</td>\n",
       "      <td>0.037684</td>\n",
       "      <td>0.002314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.095983</td>\n",
       "      <td>-0.063012</td>\n",
       "      <td>-0.116906</td>\n",
       "      <td>-0.059075</td>\n",
       "      <td>-0.051323</td>\n",
       "      <td>-0.003439</td>\n",
       "      <td>0.018687</td>\n",
       "      <td>0.006544</td>\n",
       "      <td>-0.049057</td>\n",
       "      <td>-0.031649</td>\n",
       "      <td>0.132278</td>\n",
       "      <td>-0.092611</td>\n",
       "      <td>-0.003116</td>\n",
       "      <td>-0.037459</td>\n",
       "      <td>-0.020835</td>\n",
       "      <td>-0.051365</td>\n",
       "      <td>0.009537</td>\n",
       "      <td>-0.008306</td>\n",
       "      <td>-0.036682</td>\n",
       "      <td>-0.084889</td>\n",
       "      <td>-0.070875</td>\n",
       "      <td>0.029065</td>\n",
       "      <td>-0.025502</td>\n",
       "      <td>0.016912</td>\n",
       "      <td>-0.118787</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>-0.003946</td>\n",
       "      <td>0.010542</td>\n",
       "      <td>0.007237</td>\n",
       "      <td>-0.000885</td>\n",
       "      <td>0.014610</td>\n",
       "      <td>0.081643</td>\n",
       "      <td>-0.081229</td>\n",
       "      <td>-0.025099</td>\n",
       "      <td>-0.033073</td>\n",
       "      <td>0.051276</td>\n",
       "      <td>-0.036814</td>\n",
       "      <td>-0.003755</td>\n",
       "      <td>-0.009661</td>\n",
       "      <td>0.079958</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092280</td>\n",
       "      <td>0.056203</td>\n",
       "      <td>0.071148</td>\n",
       "      <td>0.015051</td>\n",
       "      <td>0.042216</td>\n",
       "      <td>-0.075158</td>\n",
       "      <td>0.012212</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>0.019432</td>\n",
       "      <td>-0.002632</td>\n",
       "      <td>-0.042431</td>\n",
       "      <td>0.027430</td>\n",
       "      <td>0.061088</td>\n",
       "      <td>0.048532</td>\n",
       "      <td>0.014709</td>\n",
       "      <td>-0.031145</td>\n",
       "      <td>-0.082542</td>\n",
       "      <td>0.164593</td>\n",
       "      <td>-0.027809</td>\n",
       "      <td>0.045824</td>\n",
       "      <td>0.068942</td>\n",
       "      <td>0.005452</td>\n",
       "      <td>0.028891</td>\n",
       "      <td>-0.029380</td>\n",
       "      <td>0.115112</td>\n",
       "      <td>-0.027030</td>\n",
       "      <td>-0.049816</td>\n",
       "      <td>-0.002534</td>\n",
       "      <td>0.053893</td>\n",
       "      <td>0.067417</td>\n",
       "      <td>-0.041085</td>\n",
       "      <td>-0.008593</td>\n",
       "      <td>-0.021544</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>-0.019502</td>\n",
       "      <td>0.050039</td>\n",
       "      <td>-0.029175</td>\n",
       "      <td>0.005498</td>\n",
       "      <td>0.152892</td>\n",
       "      <td>0.024720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.011600</td>\n",
       "      <td>0.056510</td>\n",
       "      <td>0.016624</td>\n",
       "      <td>-0.094690</td>\n",
       "      <td>-0.009866</td>\n",
       "      <td>0.072347</td>\n",
       "      <td>0.044124</td>\n",
       "      <td>-0.041175</td>\n",
       "      <td>-0.042124</td>\n",
       "      <td>-0.102631</td>\n",
       "      <td>-0.072923</td>\n",
       "      <td>0.018687</td>\n",
       "      <td>0.027006</td>\n",
       "      <td>0.002026</td>\n",
       "      <td>0.076499</td>\n",
       "      <td>0.041676</td>\n",
       "      <td>0.060305</td>\n",
       "      <td>-0.011029</td>\n",
       "      <td>0.063772</td>\n",
       "      <td>0.099191</td>\n",
       "      <td>-0.100889</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>-0.029850</td>\n",
       "      <td>0.040438</td>\n",
       "      <td>0.082426</td>\n",
       "      <td>0.012108</td>\n",
       "      <td>-0.063109</td>\n",
       "      <td>0.068578</td>\n",
       "      <td>-0.021534</td>\n",
       "      <td>0.070561</td>\n",
       "      <td>0.052976</td>\n",
       "      <td>0.009054</td>\n",
       "      <td>-0.086911</td>\n",
       "      <td>0.054717</td>\n",
       "      <td>0.045982</td>\n",
       "      <td>-0.023860</td>\n",
       "      <td>-0.017685</td>\n",
       "      <td>0.041726</td>\n",
       "      <td>-0.097746</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055667</td>\n",
       "      <td>0.087691</td>\n",
       "      <td>-0.018529</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.042829</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>-0.015800</td>\n",
       "      <td>0.079694</td>\n",
       "      <td>-0.049632</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>-0.016577</td>\n",
       "      <td>-0.027981</td>\n",
       "      <td>0.048770</td>\n",
       "      <td>-0.074168</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.044579</td>\n",
       "      <td>0.047510</td>\n",
       "      <td>0.107275</td>\n",
       "      <td>0.041583</td>\n",
       "      <td>-0.004871</td>\n",
       "      <td>-0.019034</td>\n",
       "      <td>-0.003743</td>\n",
       "      <td>-0.054850</td>\n",
       "      <td>-0.022881</td>\n",
       "      <td>-0.019712</td>\n",
       "      <td>0.050974</td>\n",
       "      <td>0.031804</td>\n",
       "      <td>0.048750</td>\n",
       "      <td>0.020057</td>\n",
       "      <td>-0.135238</td>\n",
       "      <td>0.013612</td>\n",
       "      <td>-0.049410</td>\n",
       "      <td>-0.006925</td>\n",
       "      <td>0.085355</td>\n",
       "      <td>-0.007875</td>\n",
       "      <td>0.030402</td>\n",
       "      <td>-0.029012</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>-0.000133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2    ...       381       382       383\n",
       "0  -0.023889  0.055259 -0.011655  ...  0.005778  0.034098 -0.006889\n",
       "1  -0.012688  0.046874 -0.010502  ... -0.023103 -0.002785  0.010474\n",
       "2   0.000494  0.119412  0.005230  ...  0.016877 -0.023861  0.005264\n",
       "3  -0.029711  0.023298 -0.057041  ...  0.003617  0.033993 -0.010255\n",
       "4  -0.025628  0.070389 -0.017380  ... -0.040412  0.083346  0.026855\n",
       "5  -0.022656  0.021160  0.005105  ...  0.059089 -0.017509  0.007166\n",
       "6  -0.002911  0.060791 -0.009176  ... -0.073420  0.016235 -0.000244\n",
       "7  -0.080526  0.059888 -0.048847  ... -0.005989  0.134562  0.034817\n",
       "8  -0.034388  0.072501  0.014440  ...  0.000578 -0.025124  0.034191\n",
       "9  -0.005964  0.025044 -0.003182  ...  0.029937  0.070214  0.041565\n",
       "10 -0.039008 -0.010609 -0.007383  ... -0.083904  0.037684  0.002314\n",
       "11 -0.095983 -0.063012 -0.116906  ...  0.005498  0.152892  0.024720\n",
       "12 -0.011600  0.056510  0.016624  ... -0.029012  0.001888 -0.000133\n",
       "\n",
       "[13 rows x 384 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac270a3-ad77-482e-8f8a-0f6ca69ae21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df['texts'] = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64ddb6e7-704f-4f45-8965-a0f1cbd955f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df.to_csv(\"embed_text.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60fbcc3a-2130-4308-8db7-a5835209bd2d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a948d4d8aa004e8cba4e474137c49e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pushing the dataset to huggingface hub\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f4b5998-d12f-452d-9c37-4f71d607fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8efea99-30fc-44d8-8966-51ec2e6b19b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', 'texts'],\n",
       "    num_rows: 13\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ds = Dataset.from_pandas(embed_df)\n",
    "embed_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "238060ab-8e80-4b8d-88d0-3039bce778de",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ds = embed_ds.train_test_split(train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef82a224-7b6f-4351-95f1-082a88b24807",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', 'texts'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', 'texts'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a73fa5b-be79-4d9d-96a7-3977f6667977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b92217eaf7b4323b6fb25acafc3637b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2acb4ad4ae940a5a877167693aaf04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3010f1bd32e84f118607d0fb21f7737e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48db3df833c4cf1baeb8e5ff9809596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceddd2eabde44693b23682dfd7f5b51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path = \"Kamaljp/embed_texts\"\n",
    "embed_ds.push_to_hub(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c508eeb-dbb7-4434-a338-3bdfc84e8b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are some models with mandatory prompts,\n",
    "\n",
    "allmini = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "prompts = {\n",
    "    \"classification\": \"Classify the following text:\",\n",
    "    \"retrieval\": \"Retrieve Semantically Similar text:\",\n",
    "    \"Clustering\": \"Identify the topic or theme based on the text:\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "691cc5a2-9b1d-4711-bf72-682c4077ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiling_model = SentenceTransformer(\n",
    "    model_name_or_path=allmini,\n",
    "    prompts=prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b33c777d-25c0-44f1-9fe4-4be9af845e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# during embedding, the following process is followed\n",
    "\n",
    "embeddings = multiling_model.encode(\"There are many good looking places\",\n",
    "                                    prompt_name='retrieval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa569b8b-a111-47e0-8be6-d876880a7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the Transformers AutoClasses, will require additional steps to access the embedding\n",
    "# We cannot dismantle the model so we need to dissect the model output\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c28cf21f-3e50-4de9-af3c-c5b1bdca0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(allmini)\n",
    "model = AutoModel.from_pretrained(allmini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aaf57e21-db4b-4df0-9675-710d27f279ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"This framework generate embeddings for each input sentence\",\n",
    "    \"Sentences are passed as a list of strings\",\n",
    "    \"The quick brown fox jupms over the lazy dog.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dda8d5a6-3977-4907-a072-cca73af31f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_sentence = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e247ace0-3e95-41a6-8dcd-4a54468b2fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7bd9e2c-e061-4724-abae-89bf1d036c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When getting the inference, the encoded tokens are sent into the models for processing\n",
    "# first step in the model is embedding\n",
    "import torch\n",
    "\n",
    "with torch.no_grad():  # We don't want the model to calculate the gradient, when making this pass\n",
    "    model_out = model(**encode_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f2350af-ed46-4ce0-a35e-04068a10d777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_out)  # observe the type of the model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b5b8942-975a-4240-a1de-d64386916425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 384])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeds = model_out[0]\n",
    "token_embeds.shape  # Try to explain the shape, by relooking at the earlier steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcdb0aa-3a93-414c-93d1-a5d2d75b7524",
   "metadata": {},
   "source": [
    "#### Next mean-pooling is not mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f3a52cd-ebf5-49bf-ac3f-9d23ab2c77ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets seperate the attn_mask\n",
    "attn_mask = encode_sentence[\"attention_mask\"]\n",
    "attn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b32b1f0a-80ff-4b29-92e9-24ab7440652f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting the process of Mean Pooling manually. This step is done for model inference\n",
    "# expanding the attn_masks\n",
    "attn_mask_expanded = attn_mask.unsqueeze(-1).expand(token_embeds.size()).float()\n",
    "# attn_mask_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd93bed8-18cf-43ea-ba74-ed6d739502b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_embeds = torch.sum(token_embeds * attn_mask_expanded , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75a2aace-c775-43cc-9e5e-68dfb40a05be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_mask = torch.clamp(attn_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de5b4164-7d52-4eaa-a1ca-f384d0b8f4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0923, -0.2241, -0.0747,  ...,  0.5802,  0.6137, -0.2178],\n",
       "        [ 0.2897,  0.2838,  0.2554,  ...,  0.2671,  0.3355, -0.1703],\n",
       "        [ 0.1504,  0.4607,  0.2637,  ...,  0.0908,  0.3025,  0.3073]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled = sum_embeds / sum_mask\n",
    "mean_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b4b45-2b79-4349-8607-ce7ee083b0da",
   "metadata": {},
   "source": [
    "#### Some Tasks with Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915a3eb-cff6-4fa5-9816-4ac83c947b6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47a8315f-409b-440a-8a5d-a2a9645ddd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Similarity\n",
    "\n",
    "sentences2 = [\n",
    "    'There is more to embeddings than it meets the eyes',\n",
    "    'Every object in the Neural Network world have a rich and varied back story',\n",
    "    'when there are instances, that means there has to be blueprints of them lying around'\n",
    "]\n",
    "sentences = ['This framework generate embeddings for each input sentence',\n",
    " 'Sentences are passed as a list of strings',\n",
    " 'The quick brown fox jupms over the lazy dog.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb1b4201-c034-44e3-8029-f701be43bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model_embedding.encode(sentences)\n",
    "embed2 = model_embedding.encode(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c80cadcb-f267-4bbb-949b-b9500dda6802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3594, 0.1727, 0.1343],\n",
       "        [0.3569, 0.1764, 0.1245],\n",
       "        [0.1035, 0.1637, 0.1032]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the cosine scores\n",
    "cos_scores = util.cos_sim(embed2, embed)\n",
    "cos_scores  # the scores for each sentence in one embeding list is compared with another embeding list\n",
    "# So there will be a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81627534-564a-4cd7-93b8-2474d37d3432",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### semantic search\n",
    "\n",
    "Symmetric search: Query and the retrieved sentences are having the same length\n",
    "[link](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models)\n",
    "\n",
    "Assymetric search: Query is smaller in size, while the sentences are longer, like paras\n",
    "[link](https://www.sbert.net/docs/pretrained-models/msmarco-v3.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f8e0ff5f-12f3-43ba-a23f-34afa805d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating a piece of bread.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "    \"A woman is playing violin.\",\n",
    "    \"Two men pushed carts through the woods.\",\n",
    "    \"A man is riding a white horse on an enclosed ground.\",\n",
    "    \"A monkey is playing drums.\",\n",
    "    \"A cheetah is running behind its prey.\",\n",
    "]\n",
    "\n",
    "# Query sentences:\n",
    "queries = [\n",
    "    \"A man is eating pasta.\",\n",
    "    \"Someone in a gorilla costume is playing a set of drums.\",\n",
    "    \"A cheetah chases prey on across a field.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bfcb919d-9fe9-46d5-92f8-2576f77e6df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import (\n",
    "    normalize_embeddings,\n",
    "    dot_score,\n",
    "    semantic_search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b222148-f75b-49e7-b406-adc669f4dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = model_embedding.encode(corpus, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b239a995-955d-472d-ba09-9f6d3bdbad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings =  corpus_embeddings.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3758a283-a85a-43b0-94f9-cf5b1b257299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 384])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d58f587a-4498-4111-8744-348ee8626d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0332,  0.0044, -0.0063,  ...,  0.0692, -0.0246, -0.0376],\n",
       "        [ 0.0525,  0.0552, -0.0112,  ..., -0.0162, -0.0602, -0.0412],\n",
       "        [-0.0363, -0.0357, -0.0272,  ..., -0.0386,  0.1057, -0.0013],\n",
       "        ...,\n",
       "        [ 0.0370,  0.0226,  0.0496,  ..., -0.0031,  0.0489,  0.0167],\n",
       "        [ 0.0235, -0.0585,  0.0560,  ...,  0.0584,  0.0377,  0.0410],\n",
       "        [ 0.0228,  0.1041, -0.0340,  ...,  0.0029,  0.0386,  0.0438]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalized corpus embeddings make it simple to calculat the dot-pdt\n",
    "corpus_embeddings = normalize_embeddings(corpus_embeddings)\n",
    "corpus_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a9f002c-0d7f-41cb-858a-4d71ebe21f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0116, -0.0508, -0.0217,  ...,  0.0822,  0.0099, -0.0394],\n",
       "        [-0.0357,  0.0168,  0.0448,  ...,  0.0249,  0.0653, -0.0112],\n",
       "        [ 0.0544,  0.0540, -0.0037,  ...,  0.0325,  0.0219,  0.0621]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings = model_embedding.encode(queries, convert_to_tensor=True)\n",
    "query_embeddings = normalize_embeddings(query_embeddings.to('cuda'))\n",
    "query_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "57c9acd7-bc8c-4db0-b66d-5f071f8040c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_search = semantic_search(query_embeddings, corpus_embeddings, score_function=dot_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f579a4b-68ee-441a-a5f3-d5bd7fa0766a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'corpus_id': 0, 'score': 0.7035486698150635},\n",
       "  {'corpus_id': 1, 'score': 0.5271987318992615},\n",
       "  {'corpus_id': 3, 'score': 0.18889553844928741},\n",
       "  {'corpus_id': 6, 'score': 0.10469923168420792},\n",
       "  {'corpus_id': 8, 'score': 0.09803037345409393},\n",
       "  {'corpus_id': 7, 'score': 0.08189043402671814},\n",
       "  {'corpus_id': 4, 'score': 0.033593956381082535},\n",
       "  {'corpus_id': 5, 'score': -0.059434838593006134},\n",
       "  {'corpus_id': 2, 'score': -0.08980069309473038}],\n",
       " [{'corpus_id': 7, 'score': 0.6432533264160156},\n",
       "  {'corpus_id': 4, 'score': 0.25641557574272156},\n",
       "  {'corpus_id': 3, 'score': 0.1388726532459259},\n",
       "  {'corpus_id': 6, 'score': 0.11909151822328568},\n",
       "  {'corpus_id': 8, 'score': 0.10798682272434235},\n",
       "  {'corpus_id': 0, 'score': 0.06300687044858932},\n",
       "  {'corpus_id': 2, 'score': 0.02465788647532463},\n",
       "  {'corpus_id': 1, 'score': 0.021566985175013542},\n",
       "  {'corpus_id': 5, 'score': -0.08950325846672058}],\n",
       " [{'corpus_id': 8, 'score': 0.8253214359283447},\n",
       "  {'corpus_id': 0, 'score': 0.1398952305316925},\n",
       "  {'corpus_id': 7, 'score': 0.12919363379478455},\n",
       "  {'corpus_id': 6, 'score': 0.10974162817001343},\n",
       "  {'corpus_id': 3, 'score': 0.06497804075479507},\n",
       "  {'corpus_id': 1, 'score': 0.034424006938934326},\n",
       "  {'corpus_id': 2, 'score': 0.015606727451086044},\n",
       "  {'corpus_id': 5, 'score': -0.015542411245405674},\n",
       "  {'corpus_id': 4, 'score': -0.033420782536268234}]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69710b-5843-452c-b2d1-bf81ad26551d",
   "metadata": {},
   "source": [
    "##### Many real world problems can be solved using the above embedding models\n",
    "\n",
    "- Using ANN to search for getting the context for the RAG\n",
    "\n",
    "- Retrieve Similar questions, or similar problems, similar products based on something chosen by the user\n",
    "\n",
    "- Ranking the retrieved snippets for relevancy is another interesting task that can enrich the search process-\n",
    "\n",
    "- Clustering the sentences into topics or ideas\n",
    "\n",
    "- Paraphrase mining of large corpus texts with similar meaning or idea\n",
    "\n",
    "- Image Search using the embedded data of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cceeeff-2f02-4f53-b609-2448934c5a55",
   "metadata": {},
   "source": [
    "##### Retrieve & Re-Rank\n",
    "\n",
    "We will be using the CrossEncoder modlel cross-encoder/ms-marco-MiniLM-L-6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a420c2-ba66-44c8-9dd0-2c79b694a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "23bbb9f3-cb69-43dc-b6a4-cc580ec5a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import gzip\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65b6aa74-f8cf-4bb7-8678-f83759d6347e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7b91b8af6742f38d08f188aca0d7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32021597cc54e28802759124b523267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404a0a75f23d4300afcb1fe8324dc791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4e04d4ad0c42a3b890c184d11e1bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc0e007512c4814a77e0a39e211eaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be544eb118e64f59b4614e33f94e9c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3677a0970df1475da4dde9bf4a7f4bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a30bda2523f4c51a4d2d75e8d18cd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc57c2b77df74486a3b47ab7d7df354f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d170838c1d9e410e9b79a49ee28c8b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b52855c50e4567903b779f60538adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multi_qa = \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "bi_encoder = SentenceTransformer(multi_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "93d9a496-78e3-49f2-ab32-ee837696689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_enc = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "cross_encoder = CrossEncoder(cross_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8acaf745-aaf6-4b54-967f-9b0bfb745976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2496f4360218493cba8317403056c73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/50.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "if not os.path.exists(wikipedia_filepath):\n",
    "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5052f6b7-8cec-4e93-96ea-14bbcff894f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passages: 169597\n"
     ]
    }
   ],
   "source": [
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        #Add all paragraphs\n",
    "        #passages.extend(data['paragraphs'])\n",
    "\n",
    "        #Only add the first paragraph\n",
    "        passages.append(data['paragraphs'][0])\n",
    "\n",
    "print(\"Passages:\", len(passages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "554203a7-3506-4a2c-a015-09e2909e1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_encoder = bi_encoder.to(\"cuda\")  # In CPU will take long time, items/sec rate will be 10/ 15. With "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8fc8d750-0ccf-4b58-b6c6-3a318a9e13c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ade55317d041b7b0c2b6673440f898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# embed the entire 170K passages with bi-encoder\n",
    "corpus_embeding = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "097e7a4e-f8b0-45dd-bbc0-c328d0887c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8e30604b-22c3-4980-96c4-bfb56003688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We lower case our text and remove stop-words from indexing\n",
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c0ddec38-9498-4984-b5d2-b20ab5faa4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98f61575290455e883f81b4c32e982b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_corpus = []\n",
    "for passage in tqdm(passages):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6f38d72a-86e9-4b77-b316-a4cde10e0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will search all wikipedia articles for passages that\n",
    "# answer the query\n",
    "def search(query):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    ##### BM25 search (lexical search) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -5)[-5:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    print(\"Top-3 lexical search (BM25) hits\")\n",
    "    for hit in bm25_hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    ##### Semantic Search #####\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    question_embedding = question_embedding.cuda()\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=32)\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "    ##### Re-Ranking ##### Try to explain this part \n",
    "    # Now, score all retrieved passages with the cross_encoder\n",
    "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    # Sort results by the cross-encoder scores\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    # Output of top-5 hits from bi-encoder\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Bi-Encoder Retrieval hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    # Output of top-5 hits from re-ranker\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:3]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "740751bd-6236-47ef-8b83-b67e38a52294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: Who is the president of India\n",
      "Top-3 lexical search (BM25) hits\n",
      "\t14.509\tThe Vice President of India is the second-highest constitutional official in India, after the President.\n",
      "\t13.316\tFakhruddin Ali Ahmed was the fifth President of India from 1974 to 1977 and also the 2nd President of India to die in office.\n",
      "\t11.866\tThe President of India is the head of state of the Republic of India. The current president, Ram Nath Kovind, who was sworn in on 25 July 2017. He succeeded Pranab Mukherjee. The President resides in an estate known as the Rashtrapati Bhavan in New Delhi.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Bi-Encoder Retrieval hits\n",
      "\t0.119\tTed Cassidy (July 31, 1932 - January 16, 1979) was an American actor. He was best known for his roles as Lurch and Thing on \"The Addams Family\".\n",
      "\t0.077\tAileen Carol Wuornos Pralle (born Aileen Carol Pittman; February 29, 1956 – October 9, 2002) was an American serial killer. She was born in Rochester, Michigan. She confessed to killing six men in Florida and was executed in Florida State Prison by lethal injection for the murders. Wuornos said that the men she killed had raped her or tried to rape her while she was working as a prostitute.\n",
      "\t0.065\tA crater is a round dent on a planet. They are usually shaped like a circle or an oval. They are usually made by something like a meteor hitting the surface of a planet. Underground activity such as volcanoes or explosions can also cause them but it's not as likely.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Cross-Encoder Re-ranker hits\n",
      "\t-10.718\tMichael Te-Pei Chang (; born February 22, 1972) is an American retired tennis player. He was the youngest-ever male winner of a Grand Slam singles title. Michael Chang did this when he won the French Open in 1989 at the age of 17.\n",
      "\t-10.718\tAileen Carol Wuornos Pralle (born Aileen Carol Pittman; February 29, 1956 – October 9, 2002) was an American serial killer. She was born in Rochester, Michigan. She confessed to killing six men in Florida and was executed in Florida State Prison by lethal injection for the murders. Wuornos said that the men she killed had raped her or tried to rape her while she was working as a prostitute.\n",
      "\t-10.887\tTed Cassidy (July 31, 1932 - January 16, 1979) was an American actor. He was best known for his roles as Lurch and Thing on \"The Addams Family\".\n"
     ]
    }
   ],
   "source": [
    "search(query=\"Who is the president of India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cf983fa7-0926-4172-b93b-45134860fac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.8939\n",
      "The cat sits outside \t\t The cat plays in the garden \t\t Score: 0.6788\n",
      "I love pasta \t\t Do you like pizza? \t\t Score: 0.5096\n",
      "I love pasta \t\t The new movie is so great \t\t Score: 0.2560\n",
      "I love pasta \t\t The new movie is awesome \t\t Score: 0.2440\n",
      "A man is playing guitar \t\t The cat plays in the garden \t\t Score: 0.2105\n",
      "The new movie is awesome \t\t Do you like pizza? \t\t Score: 0.1969\n",
      "The new movie is so great \t\t Do you like pizza? \t\t Score: 0.1692\n",
      "The cat sits outside \t\t A woman watches TV \t\t Score: 0.1310\n",
      "The cat plays in the garden \t\t Do you like pizza? \t\t Score: 0.0900\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Single list of sentences - Possible tens of thousands of sentences\n",
    "sentences = [\n",
    "    \"The cat sits outside\",\n",
    "    \"A man is playing guitar\",\n",
    "    \"I love pasta\",\n",
    "    \"The new movie is awesome\",\n",
    "    \"The cat plays in the garden\",\n",
    "    \"A woman watches TV\",\n",
    "    \"The new movie is so great\",\n",
    "    \"Do you like pizza?\",\n",
    "]\n",
    "\n",
    "paraphrases = util.paraphrase_mining(model, sentences)\n",
    "\n",
    "for paraphrase in paraphrases[0:10]:\n",
    "    score, i, j = paraphrase\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bac1ad-87d1-4652-91d5-cc631d58d9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461be218-09d2-4711-a618-b74ecc946ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251191b4-fdf0-4b61-aff9-e20e48c626d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
